{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2ZPyPT3D_VG"
      },
      "outputs": [],
      "source": [
        "# %% Deep learning - Section 16.155\n",
        "#    AEs for occlusion\n",
        "\n",
        "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
        "#   > https://www.udemy.com/course/deeplearning_x\n",
        "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
        "# from code developed by the course instructor (Mike X. Cohen), while the\n",
        "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
        "# creative input from my side. If you are interested in DL (and if you are\n",
        "# reading this statement, chances are that you are), go check out the course, it\n",
        "# is singularly good.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oIWD8TazEUtR"
      },
      "outputs": [],
      "source": [
        "# %% Libraries and modules\n",
        "import numpy               as np\n",
        "import matplotlib.pyplot   as plt\n",
        "import torch\n",
        "import torch.nn            as nn\n",
        "import seaborn             as sns\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "import pandas              as pd\n",
        "import scipy.stats         as stats\n",
        "import sklearn.metrics     as skm\n",
        "import time\n",
        "import sys\n",
        "\n",
        "from torch.utils.data                 import DataLoader,TensorDataset\n",
        "from sklearn.model_selection          import train_test_split\n",
        "from google.colab                     import files\n",
        "from torchsummary                     import summary\n",
        "from scipy.stats                      import zscore\n",
        "from sklearn.decomposition            import PCA\n",
        "from IPython                          import display\n",
        "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg')\n",
        "plt.style.use('default')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E7S-U8-wV5yx"
      },
      "outputs": [],
      "source": [
        "# %% Data\n",
        "\n",
        "# Load data\n",
        "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
        "\n",
        "# Split labels from data\n",
        "labels = data[:,0]\n",
        "data   = data[:,1:]\n",
        "\n",
        "# Normalise data (original range is (0,255))\n",
        "data_norm = data / np.max(data)\n",
        "\n",
        "# Convert to tensor\n",
        "data_tensor = torch.tensor(data_norm).float()\n",
        "labels_tensor = torch.tensor(labels).long()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SX51fye-WVag"
      },
      "outputs": [],
      "source": [
        "# %% Model class\n",
        "\n",
        "def gen_model():\n",
        "\n",
        "    class mnist_AE(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            # Architecture\n",
        "            self.input  = nn.Linear(784,150)\n",
        "            self.encode = nn.Linear(150, 15)\n",
        "            self.mid    = nn.Linear( 15,150)\n",
        "            self.decode = nn.Linear(150,784)\n",
        "\n",
        "        # Forward propagation (store and return also latent layer)\n",
        "        def forward(self,x):\n",
        "\n",
        "            x = F.relu(self.input(x))\n",
        "            l = F.relu(self.encode(x))\n",
        "            x = F.relu(self.mid(l))\n",
        "            x = torch.sigmoid(self.decode(x))\n",
        "\n",
        "            return x,l\n",
        "\n",
        "    # Generate model instance\n",
        "    ANN = mnist_AE()\n",
        "\n",
        "    # Loss function\n",
        "    loss_fun = nn.MSELoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(ANN.parameters(),lr=0.001)\n",
        "\n",
        "    return ANN,loss_fun,optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoDBkaPlYLww",
        "outputId": "383342ca-5ffc-4e57-b3af-34420215cbfc"
      },
      "outputs": [],
      "source": [
        "# %% Test on some data\n",
        "\n",
        "ANN,loss_fun,optimizer = gen_model()\n",
        "\n",
        "X = data_tensor[:5,:]\n",
        "yHat = ANN(X)\n",
        "\n",
        "print('Input shape:')\n",
        "print(X.shape)\n",
        "print()\n",
        "\n",
        "print('yHat is now a tuple:')\n",
        "print(type(yHat),len(yHat))\n",
        "print()\n",
        "\n",
        "print('Shape of model output:')\n",
        "print(yHat[0].shape)\n",
        "print()\n",
        "\n",
        "print('Shape of encoding layer output:')\n",
        "print(yHat[1].shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vLh-D_o4YH-C"
      },
      "outputs": [],
      "source": [
        "# %% Function to train the model\n",
        "\n",
        "def train_model(ANN,loss_fun,optimizer):\n",
        "\n",
        "    # Parameters, inizialise vars\n",
        "    num_epochs = 10000\n",
        "    losses     = []\n",
        "\n",
        "    # Loop over epochs (no minibatch loop)\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Select only a random subset of images\n",
        "        random_i = np.random.choice(data_tensor.shape[0],size=32)\n",
        "        X        = data_tensor[random_i,:]\n",
        "\n",
        "        # Forward propagation and loss (for training we only need the final\n",
        "        # output, so select only the first element of the tuple yHat)\n",
        "        yHat = ANN(X)[0]\n",
        "        loss = loss_fun(yHat,X)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Loss in this epoch\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    return losses,ANN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "A1HWXHL6YKtf"
      },
      "outputs": [],
      "source": [
        "# %% Train and fit\n",
        "\n",
        "ANN,loss_fun,optimizer = gen_model()\n",
        "losses,ANN             = train_model(ANN,loss_fun,optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "CD-aOAxBYKrb",
        "outputId": "ce33fd59-42a8-4978-80a5-4a83f96a4d29"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig = plt.figure(figsize=(phi*5,5))\n",
        "\n",
        "plt.plot(losses,'-')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Model loss')\n",
        "plt.title('Model loss over epochs')\n",
        "\n",
        "plt.savefig('figure39_latent_code.png')\n",
        "plt.show()\n",
        "files.download('figure39_latent_code.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJNerffwYKn4",
        "outputId": "c696be0f-1e2f-403f-a224-eb2266c12725"
      },
      "outputs": [],
      "source": [
        "# %% Inspect the latent layer\n",
        "\n",
        "# Get data\n",
        "yHat,latent = ANN(data_tensor)\n",
        "\n",
        "print(yHat.shape)\n",
        "print(latent.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "Q8fExbp4YKiV",
        "outputId": "683d3f96-73d0-4c1f-ae9b-d7c925835790"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,ax = plt.subplots(1,2,figsize=(1.5*phi*5,5))\n",
        "\n",
        "ax[0].hist(latent.flatten().detach(),100)\n",
        "ax[0].set_xlabel('Latent activation value')\n",
        "ax[0].set_ylabel('Count')\n",
        "ax[0].set_title('Distribution of latent units activations')\n",
        "\n",
        "ax[1].imshow(latent.detach(),aspect='auto',vmin=0,vmax=10,cmap='plasma')\n",
        "ax[1].set_xlabel('Latent node')\n",
        "ax[1].set_ylabel('Image number')\n",
        "ax[1].set_title('All latent activations')\n",
        "\n",
        "plt.savefig('figure40_latent_code.png')\n",
        "plt.show()\n",
        "files.download('figure40_latent_code.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "YjI7wpx8aZpe"
      },
      "outputs": [],
      "source": [
        "# Compute average latent activation for each digit type\n",
        "\n",
        "# Preallocate (latent shape by 10 digits)\n",
        "latent_average = np.zeros((latent.shape[1],10))\n",
        "latent_std     = np.zeros((latent.shape[1],10))\n",
        "\n",
        "# Find pics by category, average and std of latent layer output\n",
        "for i in range(10):\n",
        "\n",
        "    digit_i             = np.where(labels==i)\n",
        "    latent_average[:,i] = torch.mean(latent[digit_i,:],axis=1).detach().numpy()\n",
        "    latent_std[:,i]     = torch.std(latent[digit_i,:],axis=1).detach().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "L2AL3E66aZm4",
        "outputId": "3a275148-df25-4be0-ca5c-a040e4949819"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "cmap = plt.cm.plasma(np.linspace(0.2,0.9,10))\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig = plt.figure(figsize=(phi*5,5))\n",
        "\n",
        "for i in range(10):\n",
        "    plt.plot(latent_average[:,i],'s-',color=cmap[i])\n",
        "\n",
        "plt.legend(range(10),loc=(1.01,.4))\n",
        "plt.xticks(range(15))\n",
        "plt.xlabel('Latent node number')\n",
        "plt.ylabel('Average activation')\n",
        "plt.title(\"Model's internal representation of the numbers\")\n",
        "\n",
        "plt.savefig('figure41_latent_code.png')\n",
        "plt.show()\n",
        "files.download('figure41_latent_code.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "jXqkzuxgaZkk"
      },
      "outputs": [],
      "source": [
        "# %% Explore a compressed space (PCA)\n",
        "\n",
        "# For the data select c=15 to match latent but also to speed up computations\n",
        "pca_data   = PCA(n_components=15).fit(data)\n",
        "pca_latent = PCA( ).fit(latent.detach())\n",
        "\n",
        "# Get projection of data onto the eigenvectors\n",
        "scores_data   = pca_data.fit_transform(data)\n",
        "scores_latent = pca_latent.fit_transform(latent.detach())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "DA9LZOMraZh6",
        "outputId": "afb177e3-4120-4be6-d4be-a760212ba664"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "# Eigenspectrum (scree plot)\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig = plt.figure(figsize=(phi*5,5))\n",
        "plt.plot(100*pca_data.explained_variance_ratio_,'s-',label='Data PCA')\n",
        "plt.plot(100*pca_latent.explained_variance_ratio_,'o-',label='Latent PCA')\n",
        "\n",
        "plt.xlabel('Components')\n",
        "plt.ylabel('Percent variance explained')\n",
        "plt.title('PCA eigenspectrum\\n(note the higher vals for the latent space)')\n",
        "plt.legend()\n",
        "\n",
        "plt.savefig('figure42_latent_code.png')\n",
        "plt.show()\n",
        "files.download('figure42_latent_code.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "EHYvrRpkescO",
        "outputId": "4487b3dd-5672-47eb-c123-f168802b3557"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "# Projections (note both the segregation and the overlap; and this is just a toy\n",
        "# example with mnist, so much for understandability)\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,ax = plt.subplots(1,2,figsize=(1.5*phi*5,5))\n",
        "\n",
        "for lab in range(10):\n",
        "    ax[0].plot(scores_data[labels==lab,0],scores_data[labels==lab,1],'o',markersize=3,alpha=.4)\n",
        "    ax[1].plot(scores_latent[labels==lab,0],scores_latent[labels==lab,1],'o',markersize=3,alpha=.4)\n",
        "\n",
        "for i in range(2):\n",
        "    ax[i].set_xlabel('Projection eigenvec. 1')\n",
        "    ax[i].set_ylabel('Projection eigenvec. 2')\n",
        "    ax[i].legend(range(10))\n",
        "\n",
        "ax[0].set_title('Data eigendecomposition')\n",
        "ax[1].set_title('Latent code eigendecomposition')\n",
        "\n",
        "plt.savefig('figure43_latent_code.png')\n",
        "plt.show()\n",
        "files.download('figure43_latent_code.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2QsWht6esZW"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 1\n",
        "#    Are you surprised that the latent activations (e.g., from the histogram) are all non-negative? Is that because of\n",
        "#    the image normalization, or what is causing those values to be all non-negative?\n",
        "\n",
        "# This one drove me crazy for a moment, then I realised we were talking about\n",
        "# the output of the layer, not the weights; now it's trivial, we used a ReLU\n",
        "# activation function so all the negative values are clipped, while the\n",
        "# non-negative vaules stay the same\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "2qj0rPQTesXH",
        "outputId": "baaa4a55-7545-4b4a-a577-2ed6ee563cb1"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 2\n",
        "#    Averages don't tell the whole story. Redraw the \"Model's internal representation\" line plot but using standard\n",
        "#    deviation instead of mean. This graph will tell you if any numbers, or units, have particularly higher variability\n",
        "#    than others. Is this the case, and does the std plot give you any more insight into the model's learned representation?\n",
        "\n",
        "# Plotting\n",
        "cmap = plt.cm.plasma(np.linspace(0.2,0.9,10))\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig = plt.figure(figsize=(phi*5,5))\n",
        "\n",
        "for i in range(10):\n",
        "    plt.plot(latent_std[:,i],'s-',color=cmap[i])\n",
        "\n",
        "plt.legend(range(10),loc=(1.01,.4))\n",
        "plt.xticks(range(15))\n",
        "plt.xlabel('Latent node number')\n",
        "plt.ylabel('Sts of activation')\n",
        "plt.title(\"Model's internal representation of the numbers\")\n",
        "\n",
        "plt.savefig('figure44_latent_code_extra2.png')\n",
        "plt.show()\n",
        "files.download('figure44_latent_code_extra2.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "WsYci2pahU02",
        "outputId": "6ed2a783-0403-423a-b6c6-7790ae4d827a"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 3\n",
        "#    The PC-space plots are tricky to interpret: This is a 15-dimensional space but 13 dimensions are projected onto two.\n",
        "#    It's possible that the numbers are better separated in other dimensions, just like a 2D photograph of someone standing\n",
        "#    behind a tree makes them inseparable whereas they are separable in the original 3D space. Modify the plot to show\n",
        "#    PC dimensions 2&3 instead of 1&2.\n",
        "\n",
        "# Plotting\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,ax = plt.subplots(1,2,figsize=(1.5*phi*5,5))\n",
        "\n",
        "for lab in range(10):\n",
        "    ax[0].plot(scores_data[labels==lab,0],scores_data[labels==lab,2],'o',markersize=3,alpha=.4)\n",
        "    ax[1].plot(scores_latent[labels==lab,0],scores_latent[labels==lab,2],'o',markersize=3,alpha=.4)\n",
        "\n",
        "for i in range(2):\n",
        "    ax[i].set_xlabel('Projection eigenvec. 1')\n",
        "    ax[i].set_ylabel('Projection eigenvec. 3')\n",
        "    ax[i].legend(range(10))\n",
        "\n",
        "ax[0].set_title('Data eigendecomposition')\n",
        "ax[1].set_title('Latent code eigendecomposition')\n",
        "\n",
        "plt.savefig('figure45_latent_code_extra3.png')\n",
        "plt.show()\n",
        "files.download('figure45_latent_code_extra3.png')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
