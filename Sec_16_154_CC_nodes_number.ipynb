{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfBCzfymcFnI"
      },
      "outputs": [],
      "source": [
        "# %% Deep learning - Section 16.154\n",
        "#    Code challenge 24: how many units?\n",
        "#\n",
        "#    1) Start from code from video 16.153 (mnist dataset)\n",
        "#    2) Vary parametrically the number of encoding units (10-500 in 12 steps),\n",
        "#       and the number of bottleneck units (5-100 in 8 steps); store the\n",
        "#       average loss over the last 3 epochs\n",
        "#    3) Train in minibatches using all the data, but without using dataloaders\n",
        "#    4) Optional: print a single line progess report as the parametric\n",
        "#       experiment goes on (only one line)\n",
        "#    5) Plot the losses in a matrix (encoder vs. bottleneck)\n",
        "\n",
        "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
        "#   > https://www.udemy.com/course/deeplearning_x\n",
        "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
        "# from code developed by the course instructor (Mike X. Cohen), while the\n",
        "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
        "# creative input from my side. If you are interested in DL (and if you are\n",
        "# reading this statement, chances are that you are), go check out the course, it\n",
        "# is singularly good.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Vh_obS12ck2M"
      },
      "outputs": [],
      "source": [
        "# %% Libraries and modules\n",
        "import numpy               as np\n",
        "import matplotlib.pyplot   as plt\n",
        "import torch\n",
        "import torch.nn            as nn\n",
        "import seaborn             as sns\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "import pandas              as pd\n",
        "import scipy.stats         as stats\n",
        "import sklearn.metrics     as skm\n",
        "import time\n",
        "import sys\n",
        "\n",
        "from torch.utils.data                 import DataLoader,TensorDataset\n",
        "from sklearn.model_selection          import train_test_split\n",
        "from google.colab                     import files\n",
        "from torchsummary                     import summary\n",
        "from scipy.stats                      import zscore\n",
        "from IPython                          import display\n",
        "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg')\n",
        "plt.style.use('default')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yVTwhFSecnd-"
      },
      "outputs": [],
      "source": [
        "# %% Data\n",
        "\n",
        "# Load data\n",
        "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
        "\n",
        "# Split labels from data\n",
        "labels = data[:,0]\n",
        "data   = data[:,1:]\n",
        "\n",
        "# Normalise data (original range is (0,255))\n",
        "data_norm = data / np.max(data)\n",
        "\n",
        "# Convert to tensor\n",
        "data_tensor = torch.tensor(data_norm).float()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "imWKXQ7Xcnbg"
      },
      "outputs": [],
      "source": [
        "# %% Model class\n",
        "\n",
        "def gen_model(n_d,n_l,lr=0.001):\n",
        "\n",
        "    class mnist_AE(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            # Architecture\n",
        "            self.input  = nn.Linear(784,n_d)\n",
        "            self.encode = nn.Linear(n_d,n_l)\n",
        "            self.mid    = nn.Linear(n_l,n_d)\n",
        "            self.decode = nn.Linear(n_d,784)\n",
        "\n",
        "        # Forward propagation (sigmoid to scale between 0 and 1)\n",
        "        def forward(self,x):\n",
        "\n",
        "            x = F.relu(self.input(x))\n",
        "            x = F.relu(self.encode(x))\n",
        "            x = F.relu(self.mid(x))\n",
        "            x = torch.sigmoid(self.decode(x))\n",
        "\n",
        "            return x\n",
        "\n",
        "    # Generate model instance\n",
        "    ANN = mnist_AE()\n",
        "\n",
        "    # Loss function\n",
        "    loss_fun = nn.MSELoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(ANN.parameters(),lr=lr)\n",
        "\n",
        "    return ANN,loss_fun,optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "FbrnB9EhgSBu"
      },
      "outputs": [],
      "source": [
        "# %% Function to train the model\n",
        "\n",
        "def train_model(ANN,loss_fun,optimizer):\n",
        "\n",
        "    # Parameters, inizialise vars\n",
        "    num_epochs = 3\n",
        "    batch_size = 32\n",
        "    n_samples  = data_tensor.shape[0]\n",
        "    losses     = []\n",
        "\n",
        "    # Loop over epochs (no minibatch loop)\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        batch_losses = []\n",
        "        batch_sizes  = []\n",
        "\n",
        "        # Select a random subset of images\n",
        "        rand_idx = np.random.permutation(data_tensor.shape[0]).astype(int)\n",
        "\n",
        "        for i in range(0,n_samples,batch_size):\n",
        "\n",
        "            # Pick a sample\n",
        "            sample = rand_idx[i:i+batch_size]\n",
        "            X      = data_tensor[sample,:]\n",
        "\n",
        "            # Forward propagation and loss (pass data themselves to loss_fun)\n",
        "            yHat = ANN(X)\n",
        "            loss = loss_fun(yHat,X)\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Batch mean loss and actual batch size (last sample might be <32)\n",
        "            batch_losses.append(loss.item())\n",
        "            batch_sizes.append(X.shape[0])\n",
        "\n",
        "        # Current epoch loss\n",
        "        losses.append(np.average(batch_losses,weights=batch_sizes))\n",
        "\n",
        "    # Average of the last three losses\n",
        "    loss_avg = np.mean(losses[-3:])\n",
        "\n",
        "    return losses,loss_avg,ANN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "U6WNlptrhLJ9"
      },
      "outputs": [],
      "source": [
        "# %% Parametric experiment\n",
        "\n",
        "# Parameters\n",
        "n_decoder  = np.linspace(10,500,12).astype(int)\n",
        "n_latent   = np.linspace(5,100,8).astype(int)\n",
        "total_exps = len(n_decoder)*len(n_latent)\n",
        "\n",
        "results = np.zeros((len(n_decoder),len(n_latent)))\n",
        "count   = 1\n",
        "\n",
        "# Loop over parameters (~15 secs per loop; ~24 mins full nodes experiment)\n",
        "for i,d in enumerate(n_decoder):\n",
        "    for j,l in enumerate(n_latent):\n",
        "\n",
        "        # Train and fit\n",
        "        ANN,loss_fun,optimizer = gen_model(d,l)\n",
        "        _,results[i,j],_       = train_model(ANN,loss_fun,optimizer)\n",
        "\n",
        "        # Update message\n",
        "        msg = \"Finished experiment {}/{}\".format(count,total_exps)\n",
        "        sys.stdout.write('\\r'+msg)\n",
        "        sys.stdout.flush()\n",
        "        count += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "c_xvptpRjLm5",
        "outputId": "a19385fd-67b5-42dc-b8ca-df9d1c176780"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig = plt.figure(figsize=(phi*5,5))\n",
        "\n",
        "extent = [n_latent[0],n_latent[-1],n_decoder[0],n_decoder[-1]]\n",
        "plt.imshow(results,aspect='auto',cmap='jet',extent=extent)\n",
        "\n",
        "dx = (n_latent[-1] - n_latent[0]) / len(n_latent)\n",
        "dy = (n_decoder[-1] - n_decoder[0]) / len(n_decoder)\n",
        "\n",
        "y_centers = n_decoder[-1] - (np.arange(len(n_decoder)) + 0.5) * dy\n",
        "x_centers = n_latent[0] + (np.arange(len(n_latent)) + 0.5) * dx\n",
        "\n",
        "plt.xticks(x_centers, n_latent)\n",
        "plt.yticks(y_centers, n_decoder)\n",
        "\n",
        "plt.xlabel('Number of bottleneck units')\n",
        "plt.ylabel('Number of encoding and deconding units')\n",
        "plt.title('Losses over nodes parametric experiment')\n",
        "plt.colorbar()\n",
        "\n",
        "plt.savefig('figure26_code_challenge_24.png')\n",
        "plt.show()\n",
        "files.download('figure26_code_challenge_24.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f1NvONN26fZ",
        "outputId": "3a20dbbf-1e00-4cf5-c21b-42e524ac25fa"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 1\n",
        "#    Because the full experiment takes a long time, it's not pratical to add another factor. Fix the number of encoder\n",
        "#    units to 100 and instead parametrically explore the learning rate. You don't need so many learning rates, just use\n",
        "#    [.0001, .001, .01]. The results can be shown in a line plot, with one line per lr and bottleneck units on the x-axis.\n",
        "\n",
        "# Parametric experiment (takes ~4 mins)\n",
        "n_decoder  = int(100)\n",
        "n_latent   = np.linspace(5,100,8).astype(int)\n",
        "l_rates    = [.0001, .001, .01]\n",
        "total_exps = len(l_rates)*len(n_latent)\n",
        "\n",
        "results_ex1 = np.zeros((len(n_latent),len(l_rates)))\n",
        "count       = 1\n",
        "\n",
        "for i,l in enumerate(n_latent):\n",
        "    for j,lr in enumerate(l_rates):\n",
        "\n",
        "        ANN,loss_fun,optimizer = gen_model(n_decoder,l,lr)\n",
        "        _,results_ex1[i,j],_   = train_model(ANN,loss_fun,optimizer)\n",
        "\n",
        "        msg = \"Finished experiment {}/{}\".format(count,total_exps)\n",
        "        sys.stdout.write('\\r'+msg)\n",
        "        sys.stdout.flush()\n",
        "        count += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "a06ZB3weM-Ty",
        "outputId": "01049802-67d5-4c4b-aea3-d3184136ec3a"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 2\n",
        "#    Continue ...\n",
        "\n",
        "# Plotting\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig = plt.figure(figsize=(phi*5,5))\n",
        "\n",
        "plt.plot(n_latent,results_ex1)\n",
        "\n",
        "plt.xticks(n_latent)\n",
        "plt.xlabel('Number of bottleneck units')\n",
        "plt.ylabel('Loss (average over last 3 epochs)')\n",
        "plt.title('Losses over nodes parametric experiment\\n(learning rates)')\n",
        "plt.legend(l_rates)\n",
        "\n",
        "plt.savefig('figure27_code_challenge_24.png')\n",
        "plt.show()\n",
        "files.download('figure27_code_challenge_24.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VU3hiJPc26Xc",
        "outputId": "4ba1eabe-0a7c-452b-b141-d6207054b4f7"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 2\n",
        "#    Smooth transitions across parameters are easy to interpret. But the image plot shows a sharp transition for small\n",
        "#    numbers of bottleneck units. This rings alarm bells for any experimental scientist! It means that something is\n",
        "#    happening at that region of parameter space and you should investigate. Thus, re-run the experiment but change the\n",
        "#    parameters to focus specifically on the region of the parameter space where there are large changes in the results.\n",
        "\n",
        "# Parametric experiment narrowing around the sharp transition (takes ~7 mins)\n",
        "n_decoder  = np.linspace(10,54,12).astype(int)\n",
        "n_latent   = np.linspace(5,19,8).astype(int)\n",
        "total_exps = len(n_decoder)*len(n_latent)\n",
        "\n",
        "results_ex2 = np.zeros((len(n_decoder),len(n_latent)))\n",
        "count       = 1\n",
        "\n",
        "for i,d in enumerate(n_decoder):\n",
        "    for j,l in enumerate(n_latent):\n",
        "\n",
        "        ANN,loss_fun,optimizer = gen_model(d,l)\n",
        "        _,results_ex2[i,j],_       = train_model(ANN,loss_fun,optimizer)\n",
        "\n",
        "        msg = \"Finished experiment {}/{}\".format(count,total_exps)\n",
        "        sys.stdout.write('\\r'+msg)\n",
        "        sys.stdout.flush()\n",
        "        count += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "8LHjvKwdTIyq",
        "outputId": "88d3642b-9d15-4497-9e00-b3e957d4999d"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 2\n",
        "#    Continue ...\n",
        "\n",
        "# Plotting\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig = plt.figure(figsize=(phi*5,5))\n",
        "\n",
        "extent = [n_latent[0],n_latent[-1],n_decoder[0],n_decoder[-1]]\n",
        "plt.imshow(results_ex2,aspect='auto',cmap='jet',extent=extent)\n",
        "\n",
        "dx = (n_latent[-1] - n_latent[0]) / len(n_latent)\n",
        "dy = (n_decoder[-1] - n_decoder[0]) / len(n_decoder)\n",
        "\n",
        "y_centers = n_decoder[-1] - (np.arange(len(n_decoder)) + 0.5) * dy\n",
        "x_centers = n_latent[0] + (np.arange(len(n_latent)) + 0.5) * dx\n",
        "\n",
        "plt.xticks(x_centers, n_latent)\n",
        "plt.yticks(y_centers, n_decoder)\n",
        "\n",
        "plt.xlabel('Number of bottleneck units')\n",
        "plt.ylabel('Number of encoding and deconding units')\n",
        "plt.title('Losses over nodes parametric experiment')\n",
        "plt.colorbar()\n",
        "\n",
        "plt.savefig('figure28_code_challenge_24_extra2.png')\n",
        "plt.show()\n",
        "files.download('figure28_code_challenge_24_extra2.png')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
