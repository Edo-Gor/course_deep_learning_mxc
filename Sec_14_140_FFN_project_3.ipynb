{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXpeehXZypvO"
      },
      "outputs": [],
      "source": [
        "# %% Deep learning - Section 14.140\n",
        "#    FFN project 3: FFN for missing data interpolation\n",
        "#    1) Use the wine quality dataset\n",
        "#    2) Choose 10 random data values in the 'residual sugar' column to replace\n",
        "#       with NaN (save removed)\n",
        "#    3) Split data into Train and Test and use the data with missing residual\n",
        "#       sugar as test\n",
        "#    4) Fit a model to predict the missing values\n",
        "#    5) Plot model performance\n",
        "#    6) Plot model-predicted data against ground truth for train and test (use a\n",
        "#       rank correlation, data likely not normal)\n",
        "\n",
        "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
        "#   > https://www.udemy.com/course/deeplearning_x\n",
        "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
        "# from code developed by the course instructor (Mike X. Cohen), while the\n",
        "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
        "# creative input from my side. If you are interested in DL (and if you are\n",
        "# reading this statement, chances are that you are), go check out the course, it\n",
        "# is singularly good.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qOF67jQJzQGb"
      },
      "outputs": [],
      "source": [
        "# %% Libraries and modules\n",
        "import numpy               as np\n",
        "import matplotlib.pyplot   as plt\n",
        "import torch\n",
        "import torch.nn            as nn\n",
        "import seaborn             as sns\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "import pandas              as pd\n",
        "import scipy.stats         as stats\n",
        "import sklearn.metrics     as skm\n",
        "import time\n",
        "\n",
        "from torch.utils.data                 import DataLoader,TensorDataset\n",
        "from sklearn.model_selection          import train_test_split\n",
        "from google.colab                     import files\n",
        "from torchsummary                     import summary\n",
        "from IPython                          import display\n",
        "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg')\n",
        "plt.style.use('default')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "e-wyjUZyKJR_"
      },
      "outputs": [],
      "source": [
        "# %% Function to get the data\n",
        "\n",
        "def get_data(url='https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv',drop_vals=10):\n",
        "\n",
        "    # Get data\n",
        "    data = pd.read_csv(url,sep=';')\n",
        "\n",
        "    # Remove some outliers (see lec. 82 for why) and 10 random residual sugar values\n",
        "    data = data[data['total sulfur dioxide']<200]\n",
        "\n",
        "    miss_idx = np.random.choice(data.index,drop_vals,replace=False)\n",
        "    true_missing_sugar = data.loc[miss_idx, 'residual sugar'].values.copy()\n",
        "    true_missing_sugar = (true_missing_sugar - true_missing_sugar.mean()) / true_missing_sugar.std()\n",
        "    data.loc[miss_idx,'residual sugar'] = np.nan\n",
        "\n",
        "    # Z-score all the variables (removed residual sugar normalised above)\n",
        "    cols2zscore = data.keys()\n",
        "\n",
        "    for col in cols2zscore:\n",
        "        mean_val  = np.mean(data[col])\n",
        "        std_val   = np.std(data[col])\n",
        "        data[col] = (data[col] - mean_val) / std_val\n",
        "\n",
        "    cols2zscore = cols2zscore.drop('residual sugar')\n",
        "\n",
        "    # Split data based on residual sugar vals\n",
        "    train_data_df = data[data['residual sugar'].notna()].copy()\n",
        "    test_data_df  = data[data['residual sugar'].isna()].copy()\n",
        "\n",
        "    # Convert from pandas dataframe to PyTorch tensor\n",
        "    train_data   = torch.tensor(train_data_df[cols2zscore].values).float()\n",
        "    train_labels = torch.tensor(train_data_df['residual sugar'].values).float().view(-1,1)\n",
        "\n",
        "    test_data    = torch.tensor(test_data_df[cols2zscore].values).float()\n",
        "    test_labels  = torch.tensor(test_data_df['residual sugar'].values).float().view(-1,1)\n",
        "\n",
        "    print(f'Train data shape:   {train_data.shape}')\n",
        "    print(f'Train labels shape: {train_labels.shape}')\n",
        "    print(f'Test data shape:    {test_data.shape}')\n",
        "    print(f'Test labels shape:  {test_labels.shape}')\n",
        "\n",
        "    # Convert into PyTorch datasets\n",
        "    train_data = TensorDataset(train_data,train_labels)\n",
        "    test_data  = TensorDataset(test_data,test_labels)\n",
        "\n",
        "    # Convert into DataLoader objects\n",
        "    batch_size   = 32\n",
        "    train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
        "    test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n",
        "\n",
        "    return train_loader,test_loader,true_missing_sugar,train_data_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPuSDjXNDO-i"
      },
      "outputs": [],
      "source": [
        "# %% Test data function\n",
        "\n",
        "train_loader,test_loader,_,_ = get_data(url,drop_vals=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ZmotFuXLD9Ek"
      },
      "outputs": [],
      "source": [
        "# %% Model class\n",
        "\n",
        "# Optional parametrised metaparameters:\n",
        "#  > number of layers and of units per layer\n",
        "#  > starting learning rate\n",
        "#  > optimizer (e.g. 'SGD', 'RMSprop', or 'Adam')\n",
        "#  > L2 regularisation\n",
        "#  > activation function (e.g., 'ReLU', 'LeakyReLU', 'ReLU6', or 'GELU')\n",
        "\n",
        "def gen_model(n_units=16,n_layers=2,lr=0.01,optim='SGD',L2_lambda=0,act_fun='ReLU'):\n",
        "\n",
        "    class model(nn.Module):\n",
        "        def __init__(self,n_units,n_layers):\n",
        "            super().__init__()\n",
        "\n",
        "            # Dictionary to store the layers and the activation function\n",
        "            self.layers  = nn.ModuleDict()\n",
        "            self.nLayers = n_layers\n",
        "            self.act_fun = act_fun\n",
        "\n",
        "            # Architecture (input, hidden, output)\n",
        "            # Input layer\n",
        "            self.layers['input'] = nn.Linear(11,n_units)\n",
        "\n",
        "            # Hidden layers\n",
        "            for i in range(n_layers):\n",
        "                self.layers[f'hidden{i}'] = nn.Linear(n_units,n_units)\n",
        "\n",
        "            # Output layer\n",
        "            self.layers['output'] = nn.Linear(n_units,1)\n",
        "\n",
        "        def forward(self,x):\n",
        "\n",
        "            # Input layer\n",
        "            x = self.layers['input'](x)\n",
        "\n",
        "            # Hidden layers (fetch selected activation function)\n",
        "            act_fun = getattr(torch.nn,self.act_fun)()\n",
        "            for i in range(self.nLayers):\n",
        "                x = act_fun(self.layers[f'hidden{i}'](x))\n",
        "\n",
        "            # Output layer\n",
        "            x = self.layers['output'](x)\n",
        "\n",
        "            return x\n",
        "\n",
        "    # Model instance, loss function, and optimizer\n",
        "    ANN       = model(n_units,n_layers)\n",
        "    loss_fun  = nn.MSELoss()\n",
        "    opti_fun  = getattr( torch.optim,optim )\n",
        "    optimizer = opti_fun(ANN.parameters(),lr=lr,weight_decay=L2_lambda)\n",
        "\n",
        "    return ANN,loss_fun,optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CBEpfOuD9DI"
      },
      "outputs": [],
      "source": [
        "# %% Test model function\n",
        "\n",
        "n_units   = 16\n",
        "n_layers  = 2\n",
        "lr        = 0.01\n",
        "optim_alg = 'Adam'\n",
        "L2_decay  = 0.01\n",
        "act_fun   = 'ReLU'\n",
        "\n",
        "ANN,loss_fun,optimizer = gen_model(n_units,n_layers,lr,optim_alg,L2_decay,act_fun)\n",
        "print(ANN)\n",
        "print(loss_fun)\n",
        "print(optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "U4hmGCusD9B5"
      },
      "outputs": [],
      "source": [
        "# %% Function to train the model\n",
        "\n",
        "# Optional parametrised metaparameters:\n",
        "#  > number of epochs\n",
        "#  > tol is hard-coded but set the tolerance to consider a prediction coorect\n",
        "\n",
        "def train_model(num_epochs=50):\n",
        "\n",
        "    # Epochs and fresh model instance\n",
        "    num_epochs = num_epochs\n",
        "    ANN,loss_fun,optimizer = gen_model(n_units,n_layers,lr,optim,L2_lambda,act_fun)\n",
        "\n",
        "    # Preallocate vars\n",
        "    train_loss  = torch.zeros(num_epochs)\n",
        "    train_psacc = torch.zeros(num_epochs)\n",
        "    test_loss   = torch.zeros(num_epochs)\n",
        "    test_psacc  = torch.zeros(num_epochs)\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_loss = []\n",
        "        batch_acc  = []\n",
        "\n",
        "        for X,y in train_loader:\n",
        "\n",
        "            # Forward pass, backpropagation, and optimizer step\n",
        "            yHat = ANN(X)\n",
        "            loss = loss_fun(yHat,y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss and pseudo-accuracy from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "            tol = 0.2\n",
        "            batch_acc.append( 100*torch.mean((torch.abs(yHat-y)<tol).float()) )\n",
        "\n",
        "        train_loss[epoch_i]  = np.mean(batch_loss).item()\n",
        "        train_psacc[epoch_i] = np.mean(batch_acc).item()\n",
        "\n",
        "        # Test loss and pseudo-accuracy (losses should be NaNs here)\n",
        "        ANN.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            X,y  = next(iter(test_loader))\n",
        "            yHat = ANN(X)\n",
        "            tol  = 0.2\n",
        "            test_loss[epoch_i]  = loss_fun(yHat,y)\n",
        "            test_psacc[epoch_i] = 100*torch.mean((torch.abs(yHat-y)<tol).float())\n",
        "\n",
        "        ANN.train()\n",
        "\n",
        "    return train_loss,train_psacc,test_loss,test_psacc,ANN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZkLQu2yD897",
        "outputId": "75ed9b0b-88b7-4e8a-95b0-bc91d3b30dc1"
      },
      "outputs": [],
      "source": [
        "# %% Test the whole setting\n",
        "\n",
        "# Generate data\n",
        "n_to_drop = 10\n",
        "train_loader,test_loader,true_missing_sugar,train_data_df = get_data(url,drop_vals=n_to_drop)\n",
        "\n",
        "# Set parameters and generate model\n",
        "n_units    = 32\n",
        "n_layers   = 2\n",
        "lr         = 0.01\n",
        "optim      = 'Adam'\n",
        "L2_lambda  = 0\n",
        "act_fun    = 'ReLU'\n",
        "num_epochs = 500\n",
        "\n",
        "ANN,loss_fun,optimizer = gen_model( n_units   = n_units,\n",
        "                                    n_layers  = n_layers,\n",
        "                                    lr        = lr,\n",
        "                                    optim     = optim,\n",
        "                                    L2_lambda = L2_lambda,\n",
        "                                    act_fun   = act_fun )\n",
        "\n",
        "# Train model\n",
        "train_loss,_,test_loss,_,ANN = train_model(num_epochs=num_epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gh7GmYSSUmkD",
        "outputId": "5909937e-71cf-4424-8bbc-8be5587394e3"
      },
      "outputs": [],
      "source": [
        "# %% Corralations between predicted and true values\n",
        "\n",
        "# Get predictions on train set\n",
        "X_train, y_train = train_loader.dataset.tensors\n",
        "with torch.no_grad():\n",
        "    y_train_pred = ANN(X_train).squeeze().numpy()\n",
        "\n",
        "y_train_true = y_train.squeeze().numpy()\n",
        "\n",
        "# Get predictions on test set (fetch ground truth vals)\n",
        "X_test, y_test = test_loader.dataset.tensors\n",
        "with torch.no_grad():\n",
        "    y_test_pred = ANN(X_test).squeeze().numpy()\n",
        "\n",
        "y_test_true = true_missing_sugar\n",
        "\n",
        "# Kolmogorov–Smirnov test for Gaussianity on raw sugar\n",
        "sugar = train_data_df['residual sugar'].values\n",
        "mu    = sugar.mean()\n",
        "sigma = sugar.std()\n",
        "\n",
        "ks_sugar = stats.kstest(sugar,'norm',args=(mu,sigma))\n",
        "print(f\"Kolmogorov–Smirnov test on raw sugar: ks = {ks_sugar.statistic:.3f}, p = {ks_sugar.pvalue:.3g}\")\n",
        "\n",
        "# Kolmogorov–Smirnov test for Gaussianity on training residuals\n",
        "train_resid = y_train_true - y_train_pred\n",
        "train_ks    = stats.kstest( (train_resid - train_resid.mean()) / train_resid.std(),'norm')\n",
        "print(f\"Kolmogorov–Smirnov test on training residuals: ks = {train_ks.statistic:.3f}, p = {train_ks.pvalue:.3g}\")\n",
        "\n",
        "# Spearman rank and Pearson correlations\n",
        "train_spear,train_spear_p = stats.spearmanr(y_train_true,y_train_pred)\n",
        "test_spear,test_spear_p   = stats.spearmanr(y_test_true,y_test_pred)\n",
        "\n",
        "train_pear,train_pear_p   = stats.pearsonr(y_train_true,y_train_pred)\n",
        "test_pear,test_pear_p     = stats.pearsonr(y_test_true,y_test_pred)\n",
        "\n",
        "print()\n",
        "print(f\"Spearman correlation (train): r = {train_spear:.3f}, p = {train_spear_p:.3g}\")\n",
        "print(f\"Spearman correlation (test):  r = {test_spear:.3f}, p = {test_spear_p:.3g}\")\n",
        "print()\n",
        "print(f\"Pearson correlation (train):  r = {train_pear:.3f}, p = {train_pear_p:.3g}\")\n",
        "print(f\"Pearson correlation (test):   r = {test_pear:.3f}, p = {test_pear_p:.3g}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "GDfrwPZcmb_k"
      },
      "outputs": [],
      "source": [
        "# %% Functions for 1D smoothing filter\n",
        "\n",
        "# Improved for edge effects - adaptive window\n",
        "def smooth_adaptive(x,k):\n",
        "    smoothed = np.zeros_like(x)\n",
        "    half_k   = k // 2\n",
        "\n",
        "    for i in range(len(x)):\n",
        "        start       = max(0, i-half_k)\n",
        "        end         = min(len(x), i+half_k + 1)\n",
        "        smoothed[i] = np.mean(x[start:end])\n",
        "\n",
        "    return smoothed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "4V8udPFMgP5I",
        "outputId": "da5609c8-0c27-4aa2-8c23-1dd0855b33a9"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig, ax = plt.subplots(1, 2, figsize=(1.5*phi*6, 6))\n",
        "\n",
        "# Train loss curve\n",
        "ax[0].plot(smooth_adaptive(train_loss.numpy(),5), label=\"Train loss\")\n",
        "ax[0].set_title(\"Training loss\")\n",
        "ax[0].set_xlabel(\"Epoch\")\n",
        "ax[0].set_ylabel(\"Loss\")\n",
        "ax[0].legend()\n",
        "ax[0].grid(alpha=0.5)\n",
        "\n",
        "# Predicted against true values\n",
        "ax[1].scatter(y_train_true,y_train_pred,alpha=0.6,label=f\"Train (ρ={train_spear:.2f})\",color=\"tab:blue\")\n",
        "ax[1].scatter(y_test_true,y_test_pred,alpha=0.8,label=f\"Test (ρ={test_spear:.2f})\",color=\"tab:red\",marker=\"^\")\n",
        "\n",
        "lims = [ min(y_train_true.min()-.25,y_test_true.min()-.25,\n",
        "             y_train_pred.min()-.25,y_test_pred.min()-.25),\n",
        "         max(y_train_true.max()+.25,y_test_true.max()+.25,\n",
        "             y_train_pred.max()+.25,y_test_pred.max()+.25) ]\n",
        "\n",
        "ax[1].plot(lims,lims,\"k--\",alpha=0.7)\n",
        "ax[1].set_xlim(lims)\n",
        "ax[1].set_ylim(lims)\n",
        "\n",
        "ax[1].set_title(f\"Predicted vs. true residual sugar (dropped vals = {n_to_drop})\")\n",
        "ax[1].set_xlabel(\"True values\")\n",
        "ax[1].set_ylabel(\"Predicted values\")\n",
        "ax[1].legend()\n",
        "ax[1].grid(alpha=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('figure8_ffn_project_3.png')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "files.download('figure8_ffn_project_3.png')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
