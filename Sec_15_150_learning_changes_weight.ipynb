{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXpeehXZypvO"
      },
      "outputs": [],
      "source": [
        "# %% Deep learning - Section 15.150\n",
        "#    Learning-related changes in weights\n",
        "\n",
        "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
        "#   > https://www.udemy.com/course/deeplearning_x\n",
        "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
        "# from code developed by the course instructor (Mike X. Cohen), while the\n",
        "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
        "# creative input from my side. If you are interested in DL (and if you are\n",
        "# reading this statement, chances are that you are), go check out the course, it\n",
        "# is singularly good.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "qOF67jQJzQGb"
      },
      "outputs": [],
      "source": [
        "# %% Libraries and modules\n",
        "import numpy               as np\n",
        "import matplotlib.pyplot   as plt\n",
        "import torch\n",
        "import torch.nn            as nn\n",
        "import seaborn             as sns\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "import pandas              as pd\n",
        "import scipy.stats         as stats\n",
        "import sklearn.metrics     as skm\n",
        "import time\n",
        "\n",
        "from torch.utils.data                 import DataLoader,TensorDataset\n",
        "from sklearn.model_selection          import train_test_split\n",
        "from google.colab                     import files\n",
        "from torchsummary                     import summary\n",
        "from scipy.stats                      import zscore\n",
        "from IPython                          import display\n",
        "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg')\n",
        "plt.style.use('default')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p2NS_KjlGVOR"
      },
      "outputs": [],
      "source": [
        "# %% Data\n",
        "\n",
        "# Load data\n",
        "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
        "\n",
        "# Split labels from data\n",
        "labels = data[:,0]\n",
        "data   = data[:,1:]\n",
        "\n",
        "# Normalise data (original range is (0,255))\n",
        "data_norm = data / np.max(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WHCN8Ql9HkbA"
      },
      "outputs": [],
      "source": [
        "# %% Create train and test datasets\n",
        "\n",
        "# Convert to tensor (float and integers)\n",
        "data_tensor   = torch.tensor(data_norm).float()\n",
        "labels_tensor = torch.tensor(labels).long()\n",
        "\n",
        "# Split data with scikitlearn (10% test data)\n",
        "train_data,test_data,train_labels,test_labels = train_test_split(data_tensor,labels_tensor,test_size=0.1)\n",
        "\n",
        "# Convert to PyTorch datasets\n",
        "train_data = TensorDataset(train_data,train_labels)\n",
        "test_data  = TensorDataset(test_data,test_labels)\n",
        "\n",
        "# Convert into DataLoader objects\n",
        "batch_size   = 32\n",
        "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
        "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "ABwKA8hrHrvY"
      },
      "outputs": [],
      "source": [
        "# %% Model class\n",
        "\n",
        "def gen_model():\n",
        "\n",
        "    class mnist_FFN(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            # Architecture\n",
        "            self.input   = nn.Linear(784,64)\n",
        "            self.hidden1 = nn.Linear(64,32)\n",
        "            self.hidden2 = nn.Linear(32,32)\n",
        "            self.output  = nn.Linear(32,10)\n",
        "\n",
        "        # Forward propagation\n",
        "        def forward(self,x):\n",
        "\n",
        "            x = F.relu(self.input(x))\n",
        "            x = F.relu(self.hidden1(x))\n",
        "            x = F.relu(self.hidden2(x))\n",
        "            x = self.output(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "    # Generate model instance\n",
        "    ANN = mnist_FFN()\n",
        "\n",
        "    # Loss function\n",
        "    loss_fun = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer (SGD and small lr for illustration purposes)\n",
        "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.001)\n",
        "\n",
        "    return ANN,loss_fun,optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "1yjRg6Fl7vRE"
      },
      "outputs": [],
      "source": [
        "# %% Function to train the model\n",
        "\n",
        "def train_model(ANN,loss_fun,optimizer):\n",
        "\n",
        "    # Parameters, inizialise vars\n",
        "    num_epochs = 60\n",
        "\n",
        "    losses    = []\n",
        "    train_acc = []\n",
        "    test_acc  = []\n",
        "\n",
        "    weight_euclidian = np.zeros((num_epochs,4))\n",
        "    weight_cond_num  = np.zeros((num_epochs,4))\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Store weights for each layer (\"pre-training\")\n",
        "        pre_w = []\n",
        "        for p in ANN.named_parameters():\n",
        "            if 'weight' in p[0]:\n",
        "                pre_w.append(copy.deepcopy(p[1].data.numpy()))\n",
        "\n",
        "        # Loop over training batches\n",
        "        batch_acc  = []\n",
        "        batch_loss = []\n",
        "\n",
        "        for X,y in train_loader:\n",
        "\n",
        "            # Forward propagation and loss\n",
        "            yHat = ANN(X)\n",
        "            loss = loss_fun(yHat,y)\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss and accuracy from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "            matches     = torch.argmax(yHat,axis=1) == y\n",
        "            matches_num = matches.float()\n",
        "            accuracy    = 100 * torch.mean(matches_num)\n",
        "            batch_acc.append(accuracy)\n",
        "\n",
        "        losses.append( np.mean(batch_loss) )\n",
        "        train_acc.append( np.mean(batch_acc) )\n",
        "\n",
        "        # Test accuracy\n",
        "        ANN.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            X,y = next(iter(test_loader))\n",
        "            yHat = ANN(X)\n",
        "            test_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
        "\n",
        "        ANN.train()\n",
        "\n",
        "        # Get weights for each layer (\"post-training\") and compute matrix change\n",
        "        # measures (Euclidean distance/Frobenius norm, and condition number)\n",
        "        for (i,p) in enumerate(ANN.named_parameters()):\n",
        "            if 'weight' in p[0]:\n",
        "\n",
        "                weight_euclidian[epoch_i,int(i/2)] = np.linalg.norm( pre_w[int(i/2)]-p[1].data.numpy(),ord='fro' )\n",
        "                weight_cond_num[epoch_i,int(i/2)]  = np.linalg.cond( p[1].data )\n",
        "\n",
        "    return train_acc,test_acc,losses,ANN,weight_euclidian,weight_cond_num,pre_w\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "GzE6JcA4-JQ5"
      },
      "outputs": [],
      "source": [
        "# %% Model instance and fitting\n",
        "\n",
        "ANN,loss_fun,optimizer = gen_model()\n",
        "train_acc,test_acc,losses,ANN,weight_euclidian,weight_cond_num,pre_w = train_model(ANN,loss_fun,optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "GeaHbLDp-JOS",
        "outputId": "65be85ab-6736-42a3-a093-a1ac6396a50f"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "# Get layer names\n",
        "layer_name = []\n",
        "for (i,p) in enumerate(ANN.named_parameters()):\n",
        "    if 'weight' in p[0]:\n",
        "        layer_name.append(p[0][:-7])\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,ax = plt.subplots(1,3,figsize=(2*phi*5,5))\n",
        "\n",
        "c_i  = len(weight_euclidian[1,:])\n",
        "cmap = plt.cm.plasma(np.linspace(0.2,0.9,c_i))\n",
        "\n",
        "# Accuracies\n",
        "ax[0].plot(train_acc)\n",
        "ax[0].plot(test_acc)\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Accuracy (%)')\n",
        "ax[0].set_title('Accuracy')\n",
        "ax[0].legend(['Train','Test'])\n",
        "\n",
        "# Frobenius norm\n",
        "for i in range(c_i):\n",
        "    ax[1].plot(weight_euclidian[:,i-1],color=cmap[i])\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_title('Frobenius norm')\n",
        "ax[1].legend(layer_name)\n",
        "\n",
        "# Condition numbers\n",
        "for i in range(c_i):\n",
        "    ax[2].plot(weight_cond_num[:,i-1],color=cmap[i])\n",
        "ax[2].set_xlabel('Epochs')\n",
        "ax[2].set_title('Condition number')\n",
        "ax[2].legend(layer_name)\n",
        "ax[2].set_ylim([0,20])\n",
        "\n",
        "plt.savefig('figure43_weight_changes.png')\n",
        "plt.show()\n",
        "files.download('figure43_weight_changes.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "Apz-dLr6-JL4",
        "outputId": "b0114024-11db-48b7-cf90-9e81e5ac82c0"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "# Check the derivative of accuracy against the weight change (zscore for scaling\n",
        "# offset)\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,ax = plt.subplots(1,figsize=(phi*5,5))\n",
        "\n",
        "plt.plot(zscore(np.diff(train_acc)),label=\"diff(train_acc)\")\n",
        "plt.plot(zscore(np.mean(weight_euclidian,axis=1)),label='Weight change')\n",
        "plt.legend()\n",
        "plt.title('Change in weights by change in accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.savefig('figure44_weight_changes.png')\n",
        "plt.show()\n",
        "files.download('figure44_weight_changes.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "kZJ4uF8s-JJO"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 1\n",
        "#    Re-run the training and visualization with L2 regularization (lambda=.01). Does that have a major noticeable effect?\n",
        "\n",
        "# Not much, maybe the learning is a bit smoother ? If anything the Frobenius\n",
        "# norm distribution seems to have a larger variance\n",
        "\n",
        "# Model class with L2\n",
        "def gen_model():\n",
        "\n",
        "    class mnist_FFN(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            self.input   = nn.Linear(784,64)\n",
        "            self.hidden1 = nn.Linear(64,32)\n",
        "            self.hidden2 = nn.Linear(32,32)\n",
        "            self.output  = nn.Linear(32,10)\n",
        "\n",
        "        def forward(self,x):\n",
        "\n",
        "            x = F.relu(self.input(x))\n",
        "            x = F.relu(self.hidden1(x))\n",
        "            x = F.relu(self.hidden2(x))\n",
        "            x = self.output(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "    ANN       = mnist_FFN()\n",
        "    loss_fun  = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.001,weight_decay=0.01)\n",
        "\n",
        "    return ANN,loss_fun,optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "RAgeYfP2LBn6"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 2\n",
        "#    Then try with L1 regularization. (Hint: you might want to copy code from DUDL_overfitting_L1regu).\n",
        "\n",
        "# Similar, but the Frobenius norm distribution is somewhat pointier; remarkable\n",
        "# given that indeed L1 reg. uses the Euclidean norm, while L2 reg. use the\n",
        "# squared version (i.e. L1 puts a cost on any w that is not zero, while L2 puts\n",
        "# more cost on larger weights)\n",
        "\n",
        "# Function to train the model with L1\n",
        "def train_model(ANN,loss_fun,optimizer):\n",
        "\n",
        "    num_epochs = 60\n",
        "\n",
        "    losses    = []\n",
        "    train_acc = []\n",
        "    test_acc  = []\n",
        "\n",
        "    weight_euclidian = np.zeros((num_epochs,4))\n",
        "    weight_cond_num  = np.zeros((num_epochs,4))\n",
        "\n",
        "    # Count weight number\n",
        "    n_weights = 0\n",
        "    for pname,weights in ANN.named_parameters():\n",
        "        if 'bias' not in pname:\n",
        "            n_weights = n_weights + weights.numel()\n",
        "\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        pre_w = []\n",
        "        for p in ANN.named_parameters():\n",
        "            if 'weight' in p[0]:\n",
        "                pre_w.append(copy.deepcopy(p[1].data.numpy()))\n",
        "\n",
        "        batch_acc  = []\n",
        "        batch_loss = []\n",
        "\n",
        "        for X,y in train_loader:\n",
        "\n",
        "            yHat = ANN(X)\n",
        "            loss = loss_fun(yHat,y)\n",
        "\n",
        "            # Add L1 regularisation\n",
        "            L1_term   = torch.tensor(0.,requires_grad=True)\n",
        "            L1_lambda = 0.001\n",
        "\n",
        "            for pname,weight in ANN.named_parameters():\n",
        "                if 'bias' not in pname:\n",
        "                    L1_term = L1_term + torch.sum(torch.abs(weight))\n",
        "\n",
        "            loss = loss + (L1_lambda*L1_term)/n_weights\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "            matches     = torch.argmax(yHat,axis=1) == y\n",
        "            matches_num = matches.float()\n",
        "            accuracy    = 100 * torch.mean(matches_num)\n",
        "            batch_acc.append(accuracy)\n",
        "\n",
        "        losses.append( np.mean(batch_loss) )\n",
        "        train_acc.append( np.mean(batch_acc) )\n",
        "\n",
        "        ANN.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            X,y = next(iter(test_loader))\n",
        "            yHat = ANN(X)\n",
        "            test_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
        "\n",
        "        ANN.train()\n",
        "\n",
        "        for (i,p) in enumerate(ANN.named_parameters()):\n",
        "            if 'weight' in p[0]:\n",
        "\n",
        "                weight_euclidian[epoch_i,int(i/2)] = np.linalg.norm( pre_w[int(i/2)]-p[1].data.numpy(),ord='fro' )\n",
        "                weight_cond_num[epoch_i,int(i/2)]  = np.linalg.cond( p[1].data )\n",
        "\n",
        "    return train_acc,test_acc,losses,ANN,weight_euclidian,weight_cond_num,pre_w\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
