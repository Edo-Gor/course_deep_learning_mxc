{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXpeehXZypvO"
      },
      "outputs": [],
      "source": [
        "# %% Deep learning - Section 14.138\n",
        "#    FFN project 1: predicting heart disease\n",
        "#    1) Use a reduced version of the UCI heart disease dataset\n",
        "#    2) Clean and preprocess the data\n",
        "#    3) Build a FFN to predict disease (binarise)\n",
        "#    4) Focus on getting it to work, rather than on maximising accuracy\n",
        "\n",
        "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
        "#   > https://www.udemy.com/course/deeplearning_x\n",
        "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
        "# from code developed by the course instructor (Mike X. Cohen), while the\n",
        "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
        "# creative input from my side. If you are interested in DL (and if you are\n",
        "# reading this statement, chances are that you are), go check out the course, it\n",
        "# is singularly good.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "qOF67jQJzQGb"
      },
      "outputs": [],
      "source": [
        "# %% Libraries and modules\n",
        "import numpy               as np\n",
        "import matplotlib.pyplot   as plt\n",
        "import torch\n",
        "import torch.nn            as nn\n",
        "import seaborn             as sns\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "import pandas              as pd\n",
        "import scipy.stats         as stats\n",
        "import sklearn.metrics     as skm\n",
        "import time\n",
        "\n",
        "from torch.utils.data                 import DataLoader,TensorDataset\n",
        "from sklearn.model_selection          import train_test_split\n",
        "from google.colab                     import files\n",
        "from torchsummary                     import summary\n",
        "from IPython                          import display\n",
        "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg')\n",
        "plt.style.use('default')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "OZ5p_c3homMT"
      },
      "outputs": [],
      "source": [
        "# %% Get the data\n",
        "\n",
        "def get_data(url='https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data'):\n",
        "\n",
        "    # Get data and set colummn names\n",
        "    data = pd.read_csv(url,sep=',',header=None)\n",
        "    data.columns = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','disease']\n",
        "\n",
        "    # Look for \"?\" or other placeholders, concert to NaNs, and set to numeric\n",
        "    data = data.replace('?',np.nan)\n",
        "    data = data.apply(pd.to_numeric)\n",
        "\n",
        "    # Fill numeric NaNs with median (too few observation to loose some of them)\n",
        "    for col in data.columns:\n",
        "        data[col] = data[col].fillna(data[col].median())\n",
        "\n",
        "    # Z-score all the variables but quality\n",
        "    cols2zscore = data.keys()\n",
        "    cols2zscore = cols2zscore.drop('disease')\n",
        "\n",
        "    for col in cols2zscore:\n",
        "        mean_val  = np.mean(data[col])\n",
        "        std_val   = np.std(data[col])\n",
        "        data[col] = (data[col] - mean_val) / std_val\n",
        "\n",
        "    # Binarise quality\n",
        "    data.loc[:,'boolean_disease'] = 0\n",
        "    data.loc[data['disease']>=1, 'boolean_disease'] = 1\n",
        "    data.loc[data['disease']==0, 'boolean_disease'] = 0 # Implicit but here for clarity\n",
        "\n",
        "    # Convert from pandas dataframe to PyTorch tensor\n",
        "    data_T   = torch.tensor( data[cols2zscore].values ).float()\n",
        "    labels_T = torch.tensor( data['boolean_disease'].values ).long().view(-1,1)\n",
        "\n",
        "    print(f'Data shape: {data_T.shape}')\n",
        "    print(f'Labels shape: {labels_T.shape}')\n",
        "\n",
        "    # Split data with scikitlearn (train, dev, test)\n",
        "    train_data,tmp_data, train_labels,tmp_labels = train_test_split(data_T,labels_T,test_size=0.2)\n",
        "    dev_data,test_data, dev_labels,test_labels   = train_test_split(tmp_data,tmp_labels,test_size=0.5)\n",
        "\n",
        "    # PyTorch datasets\n",
        "    train_data = TensorDataset(train_data,train_labels)\n",
        "    dev_data   = TensorDataset(dev_data,dev_labels)\n",
        "    test_data  = TensorDataset(test_data,test_labels)\n",
        "\n",
        "    # DataLoader objects (dot drop last, again, too few observation to loose some of them)\n",
        "    batch_size   = 16\n",
        "    train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
        "    dev_loader   = DataLoader(dev_data,batch_size=dev_data.tensors[0].shape[0])\n",
        "    test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n",
        "\n",
        "    return train_loader,dev_loader,test_loader,data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 813
        },
        "id": "wXM3FYLZ0XY7",
        "outputId": "8a5af3b5-b3df-4339-e150-317966b24c74"
      },
      "outputs": [],
      "source": [
        "# %% Test data function and visualise\n",
        "\n",
        "# Data\n",
        "train_loader,dev_loader,test_loader,data = get_data()\n",
        "\n",
        "# Plotting\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "data.hist(bins=20,figsize=(phi*8,8))\n",
        "plt.suptitle(\"Feature distributions (z-scored)\")\n",
        "plt.tight_layout(rect=[0,0,1,0.97])\n",
        "\n",
        "plt.savefig('figure3_ffn_project_2.png')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "files.download('figure3_ffn_project_2.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "9uRwtaTq2IUQ"
      },
      "outputs": [],
      "source": [
        "# %% Model class\n",
        "\n",
        "# Optional parametrised metaparameters:\n",
        "#  > number of layers and of units per layer\n",
        "#  > starting learning rate\n",
        "#  > optimizer (e.g. 'SGD', 'RMSprop', or 'Adam')\n",
        "#  > L2 regularisation\n",
        "#  > activation function (e.g., 'ReLU', 'LeakyReLU', 'ReLU6', or 'GELU')\n",
        "\n",
        "def gen_model(n_units=16,n_layers=2,lr=0.01,optim='SGD',L2_lambda=0,act_fun='ReLU'):\n",
        "\n",
        "    class model(nn.Module):\n",
        "        def __init__(self,n_units,n_layers):\n",
        "            super().__init__()\n",
        "\n",
        "            # Dictionary to store the layers and the activation function\n",
        "            self.layers  = nn.ModuleDict()\n",
        "            self.nLayers = n_layers\n",
        "            self.act_fun = act_fun\n",
        "\n",
        "            # Architecture (input, hidden, output)\n",
        "            # Input layer\n",
        "            self.layers['input'] = nn.Linear(13,n_units)\n",
        "\n",
        "            # Hidden layers\n",
        "            for i in range(n_layers):\n",
        "                self.layers[f'hidden{i}'] = nn.Linear(n_units,n_units)\n",
        "\n",
        "            # Output layer\n",
        "            self.layers['output'] = nn.Linear(n_units,3)\n",
        "\n",
        "        def forward(self,x):\n",
        "\n",
        "            # Input layer\n",
        "            x = self.layers['input'](x)\n",
        "\n",
        "            # Hidden layers (fetch selected activation function)\n",
        "            act_fun = getattr(torch.nn,self.act_fun)()\n",
        "            for i in range(self.nLayers):\n",
        "                x = act_fun(self.layers[f'hidden{i}'](x))\n",
        "\n",
        "            # Output layer\n",
        "            x = self.layers['output'](x)\n",
        "\n",
        "            return x\n",
        "\n",
        "    # Model instance, loss function, and optimizer\n",
        "    ANN       = model(n_units,n_layers)\n",
        "    loss_fun  = nn.CrossEntropyLoss()\n",
        "    opti_fun  = getattr( torch.optim,optim )\n",
        "    optimizer = opti_fun(ANN.parameters(),lr=lr,weight_decay=L2_lambda)\n",
        "\n",
        "    return ANN,loss_fun,optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NgwXmWf52IRr",
        "outputId": "4a1f22c6-0706-4d9f-fe8f-c2010ec86b19"
      },
      "outputs": [],
      "source": [
        "# %% Test model function\n",
        "\n",
        "n_units   = 16\n",
        "n_layers  = 2\n",
        "lr        = 0.01\n",
        "optim_alg = 'Adam'\n",
        "L2_decay  = 0.01\n",
        "act_fun   = 'ReLU'\n",
        "\n",
        "ANN,loss_fun,optimizer = gen_model(n_units,n_layers,lr,optim_alg,L2_decay,act_fun)\n",
        "print(ANN)\n",
        "print(loss_fun)\n",
        "print(optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "q0gja3zE2IPj"
      },
      "outputs": [],
      "source": [
        "# %% Function to train the model\n",
        "\n",
        "# Optional parametrised metaparameters:\n",
        "#  > number of epochs\n",
        "\n",
        "def train_model(num_epochs=50):\n",
        "\n",
        "    # Epochs and fresh model instance\n",
        "    num_epochs = num_epochs\n",
        "    ANN,loss_fun,optimizer = gen_model(n_units,n_layers,lr,optim,L2_lambda,act_fun)\n",
        "\n",
        "    # Preallocate vars\n",
        "    train_loss = torch.zeros(num_epochs)\n",
        "    train_acc  = torch.zeros(num_epochs)\n",
        "    dev_loss   = torch.zeros(num_epochs)\n",
        "    dev_acc    = torch.zeros(num_epochs)\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_loss = []\n",
        "        batch_acc  = []\n",
        "\n",
        "        for X,y in train_loader:\n",
        "\n",
        "            # Forward pass, backpropagation, and optimizer step\n",
        "            yHat = ANN(X)\n",
        "            loss = loss_fun(yHat,y.squeeze())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss and accuracy from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "            batch_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y.squeeze()).float()) )\n",
        "\n",
        "        train_loss[epoch_i]  = np.mean(batch_loss).item()\n",
        "        train_acc[epoch_i]   = np.mean(batch_acc).item()\n",
        "\n",
        "        # Test loss and pseudo-accuracy\n",
        "        ANN.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            X,y  = next(iter(dev_loader))\n",
        "            yHat = ANN(X)\n",
        "            dev_loss[epoch_i] = loss_fun(yHat,y.squeeze())\n",
        "            dev_acc[epoch_i]  = 100*torch.mean((torch.argmax(yHat,axis=1)==y.squeeze()).float())\n",
        "\n",
        "        ANN.train()\n",
        "\n",
        "    return train_loss,train_acc,dev_loss,dev_acc,ANN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COo03wmm2INK",
        "outputId": "a2627ee0-5ba4-4767-e657-25d69c48f478"
      },
      "outputs": [],
      "source": [
        "# %% Test the whole setting\n",
        "\n",
        "# Generate data\n",
        "train_loader,dev_loader,test_loader,data = get_data()\n",
        "\n",
        "# Set parameters and generate model\n",
        "n_units    = 32\n",
        "n_layers   = 2\n",
        "lr         = 0.01\n",
        "optim      = 'SGD'\n",
        "L2_lambda  = 0\n",
        "act_fun    = 'ReLU'\n",
        "num_epochs = 250\n",
        "\n",
        "ANN,loss_fun,optimizer = gen_model( n_units   = n_units,\n",
        "                                    n_layers  = n_layers,\n",
        "                                    lr        = lr,\n",
        "                                    optim     = optim,\n",
        "                                    L2_lambda = L2_lambda,\n",
        "                                    act_fun   = act_fun )\n",
        "\n",
        "# Train model\n",
        "train_loss,train_acc,dev_loss,dev_acc,ANN = train_model(num_epochs=num_epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "Owk1YMUuC1lO"
      },
      "outputs": [],
      "source": [
        "# %% Functions for 1D smoothing filter\n",
        "\n",
        "# Improved for edge effects - adaptive window\n",
        "def smooth_adaptive(x,k):\n",
        "    smoothed = np.zeros_like(x)\n",
        "    half_k   = k // 2\n",
        "\n",
        "    for i in range(len(x)):\n",
        "        start       = max(0, i-half_k)\n",
        "        end         = min(len(x), i+half_k + 1)\n",
        "        smoothed[i] = np.mean(x[start:end])\n",
        "\n",
        "    return smoothed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "-9sDvRvH2IK0",
        "outputId": "34374bb4-412c-4637-d4f6-9cd438244422"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,axs = plt.subplots(1,2,figsize=(1.5*phi*6,6))\n",
        "\n",
        "# Train loss\n",
        "l1 = axs[0].plot(smooth_adaptive(train_loss.numpy(),10),label=\"Loss\")[0]\n",
        "axs[0].set_ylim(0.2,1)\n",
        "axs[0].set_title(f\"Training set loss and accuracy\\nFinal acc ≈ {train_acc[-5:].mean().item():.1f}%\")\n",
        "axs[0].set_xlabel(\"Epoch\")\n",
        "axs[0].set_ylabel(\"Loss\")\n",
        "\n",
        "ax0b = axs[0].twinx()\n",
        "l2 = ax0b.plot(smooth_adaptive(train_acc.numpy(),10),label=\"Accuracy\",color='tab:orange')[0]\n",
        "ax0b.set_ylim(0,102)\n",
        "\n",
        "axs[0].legend(handles=[l1,l2],loc='center right')\n",
        "\n",
        "# Dev loss\n",
        "l3 = axs[1].plot(smooth_adaptive(dev_loss.numpy(),10),label=\"Loss\")[0]\n",
        "axs[1].set_ylim(0.2,1)\n",
        "axs[1].set_title(f\"Development set loss and accuracy\\nFinal acc ≈ {dev_acc[-5:].mean().item():.1f}%\")\n",
        "axs[1].set_xlabel(\"Epoch\")\n",
        "\n",
        "ax1b = axs[1].twinx()\n",
        "l4 = ax1b.plot(smooth_adaptive(dev_acc.numpy(),10),label=\"Accuracy\",color='tab:orange')[0]\n",
        "ax1b.set_ylim(0,102)\n",
        "ax1b.set_ylabel(\"Accuracy\")\n",
        "\n",
        "axs[1].legend(handles=[l3,l4],loc='center right')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('figure4_ffn_project_2.png')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "files.download('figure4_ffn_project_2.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "DfJ4MLWsJctp",
        "outputId": "721d8dca-a5bf-459e-d631-38e68f6af934"
      },
      "outputs": [],
      "source": [
        "# %% Evaluate on test set and plot\n",
        "\n",
        "# Evaluate on test set\n",
        "ANN.eval()\n",
        "with torch.no_grad():\n",
        "    X,y    = next(iter(test_loader))\n",
        "    y_pred = torch.argmax(ANN(X),dim=1).numpy()\n",
        "\n",
        "y_true  = y.numpy().ravel()\n",
        "correct = y_pred == y_true\n",
        "test_acc = 100*np.mean(correct)\n",
        "\n",
        "# Plotting\n",
        "df_plot = pd.DataFrame({\n",
        "            \"Class\": [\"Healthy\" if c==0 else \"Pathological\" for c in y_true],\n",
        "            \"Sample index\": np.arange(len(y_true)),\n",
        "            \"Correct\": correct })\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "plt.figure(figsize=(phi*5,5))\n",
        "\n",
        "sns.swarmplot(\n",
        "        x=\"Class\",\n",
        "        y=\"Sample index\",\n",
        "        data=df_plot,\n",
        "        hue=\"Correct\",\n",
        "        palette={True:\"green\",False:\"red\"},\n",
        "        dodge=False,\n",
        "        size=8 )\n",
        "\n",
        "plt.ylabel(\"Subject index\")\n",
        "plt.title(f\"Test set predictions (accuracy = {test_acc:.1f}%)\")\n",
        "plt.legend(title=\"Predictions\")\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('figure5_ffn_project_2.png')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "files.download('figure5_ffn_project_2.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "bhII91tUQ6vV",
        "outputId": "b67740b1-c7df-4e63-f284-5d564492b8e2"
      },
      "outputs": [],
      "source": [
        "# %% Compute confusion matrices and plot\n",
        "\n",
        "# Get confusion matrices\n",
        "ANN.eval()\n",
        "with torch.no_grad():\n",
        "    train_preds  = torch.argmax(ANN(train_loader.dataset.tensors[0]),dim=1).numpy()\n",
        "    train_labels = train_loader.dataset.tensors[1].numpy()\n",
        "\n",
        "    dev_preds    = torch.argmax(ANN(dev_loader.dataset.tensors[0]),dim=1).numpy()\n",
        "    dev_labels   = dev_loader.dataset.tensors[1].numpy()\n",
        "\n",
        "    test_preds   = torch.argmax(ANN(test_loader.dataset.tensors[0]),dim=1).numpy()\n",
        "    test_labels  = test_loader.dataset.tensors[1].numpy()\n",
        "\n",
        "train_conf = skm.confusion_matrix(train_labels,train_preds)\n",
        "dev_conf   = skm.confusion_matrix(dev_labels,dev_preds)\n",
        "test_conf  = skm.confusion_matrix(test_labels,test_preds)\n",
        "\n",
        "# Plotting\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,ax = plt.subplots(1,3,figsize=(1.5*phi*6,6))\n",
        "cmap = plt.cm.Blues\n",
        "\n",
        "labels = [\"Healthy\", \"Pathological\"]\n",
        "\n",
        "# Train\n",
        "vmax = train_conf.max()\n",
        "im   = ax[0].imshow(train_conf,cmap=cmap,vmax=vmax)\n",
        "ax[0].set_xticks([0,1])\n",
        "ax[0].set_yticks([0,1])\n",
        "ax[0].set_xticklabels(labels)\n",
        "ax[0].set_yticklabels(labels)\n",
        "ax[0].set_xlabel('Predicted')\n",
        "ax[0].set_ylabel('True')\n",
        "ax[0].set_title('Train confusion matrix')\n",
        "for i in range(train_conf.shape[0]):\n",
        "    row_sum = train_conf[i].sum()\n",
        "    for j in range(train_conf.shape[1]):\n",
        "        color = 'white' if train_conf[i,j]/vmax > 0.5 else 'black'\n",
        "        ax[0].text(j,i-0.05,f\"{train_conf[i,j]}\",ha='center',va='center',color=color,fontsize=12)\n",
        "        ax[0].text(j,i+0.05,f\"({100*train_conf[i,j]/row_sum:.1f}%)\",ha='center',va='center',color=color,fontsize=10)\n",
        "\n",
        "# Dev\n",
        "vmax = dev_conf.max()\n",
        "im   = ax[1].imshow(dev_conf,cmap=cmap,vmax=vmax)\n",
        "ax[1].set_xticks([0,1])\n",
        "ax[1].set_yticks([0,1])\n",
        "ax[1].set_xticklabels(labels)\n",
        "ax[1].set_yticklabels(labels)\n",
        "ax[1].set_xlabel('Predicted')\n",
        "ax[1].set_ylabel('True')\n",
        "ax[1].set_title('Dev confusion matrix')\n",
        "for i in range(dev_conf.shape[0]):\n",
        "    row_sum = dev_conf[i].sum()\n",
        "    for j in range(dev_conf.shape[1]):\n",
        "        color = 'white' if dev_conf[i,j]/vmax > 0.5 else 'black'\n",
        "        ax[1].text(j,i-0.05,f\"{dev_conf[i,j]}\",ha='center',va='center',color=color,fontsize=12)\n",
        "        ax[1].text(j,i+0.05,f\"({100*dev_conf[i,j]/row_sum:.1f}%)\",ha='center',va='center',color=color,fontsize=10)\n",
        "\n",
        "# Test\n",
        "vmax = test_conf.max()\n",
        "im   = ax[2].imshow(test_conf,cmap=cmap,vmax=vmax)\n",
        "ax[2].set_xticks([0,1])\n",
        "ax[2].set_yticks([0,1])\n",
        "ax[2].set_xticklabels(labels)\n",
        "ax[2].set_yticklabels(labels)\n",
        "ax[2].set_xlabel('Predicted')\n",
        "ax[2].set_ylabel('True')\n",
        "ax[2].set_title('Test confusion matrix')\n",
        "for i in range(test_conf.shape[0]):\n",
        "    row_sum = test_conf[i].sum()\n",
        "    for j in range(test_conf.shape[1]):\n",
        "        color = 'white' if test_conf[i,j]/vmax > 0.5 else 'black'\n",
        "        ax[2].text(j,i-0.05,f\"{test_conf[i,j]}\",ha='center',va='center',color=color,fontsize=12)\n",
        "        ax[2].text(j,i+0.05,f\"({100*test_conf[i,j]/row_sum:.1f}%)\",ha='center',va='center',color=color,fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('figure6_ffn_project_2.png')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "files.download('figure6_ffn_project_2.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "YBBrlgaSQ6le"
      },
      "outputs": [],
      "source": [
        "# %% Compute performance measures on train and test data\n",
        "\n",
        "# Preallocate\n",
        "train_metrics = np.zeros(4)\n",
        "dev_metrics   = np.zeros(4)\n",
        "test_metrics  = np.zeros(4)\n",
        "\n",
        "# Train performance measures\n",
        "train_metrics[0] = skm.accuracy_score (train_labels,train_preds)\n",
        "train_metrics[1] = skm.precision_score(train_labels,train_preds,average='weighted')\n",
        "train_metrics[2] = skm.recall_score   (train_labels,train_preds,average='weighted')\n",
        "train_metrics[3] = skm.f1_score       (train_labels,train_preds,average='weighted')\n",
        "\n",
        "# Dev performance measures\n",
        "dev_metrics[0] = skm.accuracy_score (dev_labels,dev_preds)\n",
        "dev_metrics[1] = skm.precision_score(dev_labels,dev_preds,average='weighted')\n",
        "dev_metrics[2] = skm.recall_score   (dev_labels,dev_preds,average='weighted')\n",
        "dev_metrics[3] = skm.f1_score       (dev_labels,dev_preds,average='weighted')\n",
        "\n",
        "# Test performance measures\n",
        "test_metrics[0] = skm.accuracy_score (test_labels,test_preds)\n",
        "test_metrics[1] = skm.precision_score(test_labels,test_preds,average='weighted')\n",
        "test_metrics[2] = skm.recall_score   (test_labels,test_preds,average='weighted')\n",
        "test_metrics[3] = skm.f1_score       (test_labels,test_preds,average='weighted')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "AuxxQosSVlYZ",
        "outputId": "622daa3b-d921-4f4a-d5e1-c96dae295012"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig = plt.figure(figsize=(6*phi,6))\n",
        "\n",
        "datasets    = ['Train','Dev','Test']\n",
        "metrics     = ['Accuracy','Precision','Recall','F1-score']\n",
        "all_metrics = np.stack([train_metrics,dev_metrics,test_metrics],axis=0)\n",
        "\n",
        "x     = np.arange(len(datasets))\n",
        "width = 0.15\n",
        "\n",
        "for i in range(len(metrics)):\n",
        "    plt.bar(x+i*0.5*width-.5*width,all_metrics[:,i],width,label=metrics[i],zorder=2)\n",
        "\n",
        "plt.xticks(x,datasets)\n",
        "plt.ylim([0.84,0.92])\n",
        "plt.ylabel('Score')\n",
        "plt.title('Performance metrics per set')\n",
        "plt.legend()\n",
        "plt.grid(alpha=1,axis='y')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('figure7_ffn_project_2.png')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "files.download('figure7_ffn_project_2.png')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
