{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUNd0bnSPkJw"
      },
      "outputs": [],
      "source": [
        "# %% Deep learning - Section 15.147\n",
        "#    Code challenge 22: Xavier vs. Kaiming\n",
        "#\n",
        "#    1) Start from code from video 13.131 (wine dataset)\n",
        "#    2) Model wine quality with both Xavier and Kaiming inits (keep default\n",
        "#       bias init)\n",
        "#    3) Run both models 10 times, and run a t-test to compare (average from last\n",
        "#       five epochs)\n",
        "#    4) Plot losses, train, and test performance for one run\n",
        "#    5) Plot losses, train, and test performance distributions from 10 runs\n",
        "\n",
        "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
        "#   > https://www.udemy.com/course/deeplearning_x\n",
        "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
        "# from code developed by the course instructor (Mike X. Cohen), while the\n",
        "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
        "# creative input from my side. If you are interested in DL (and if you are\n",
        "# reading this statement, chances are that you are), go check out the course, it\n",
        "# is singularly good.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "o2_4fR9-Pyrt"
      },
      "outputs": [],
      "source": [
        "# %% Libraries and modules\n",
        "import numpy               as np\n",
        "import matplotlib.pyplot   as plt\n",
        "import torch\n",
        "import torch.nn            as nn\n",
        "import seaborn             as sns\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "import pandas              as pd\n",
        "import scipy.stats         as stats\n",
        "import sklearn.metrics     as skm\n",
        "import time\n",
        "\n",
        "from torch.utils.data                 import DataLoader,TensorDataset\n",
        "from sklearn.model_selection          import train_test_split\n",
        "from google.colab                     import files\n",
        "from torchsummary                     import summary\n",
        "from IPython                          import display\n",
        "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg')\n",
        "plt.style.use('default')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wz_fa4wx9-4l",
        "outputId": "4618c531-3420-40d9-8e45-62bcff68ae16"
      },
      "outputs": [],
      "source": [
        "# %% Load and prepare data\n",
        "\n",
        "# Load\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "data = pd.read_csv(url,sep=';')\n",
        "\n",
        "# Remove some outliers (see lec. 82 for why)\n",
        "data = data[data['total sulfur dioxide']<200]\n",
        "\n",
        "# Z-score all the variables but quality\n",
        "cols2zscore = data.keys()\n",
        "cols2zscore = cols2zscore.drop('quality')\n",
        "\n",
        "for col in cols2zscore:\n",
        "    mean_val  = np.mean(data[col])\n",
        "    std_val   = np.std(data[col])\n",
        "    data[col] = (data[col] - mean_val) / std_val\n",
        "\n",
        "# Binarise quality\n",
        "data.loc[:,'boolean_quality'] = 0\n",
        "data.loc[data['quality']>5, 'boolean_quality'] = 1\n",
        "data.loc[data['quality']<6, 'boolean_quality'] = 0 # Implicit but here for clarity\n",
        "\n",
        "# Convert from pandas dataframe to PyTorch tensor\n",
        "data_t = torch.tensor( data[cols2zscore].values ).float()\n",
        "labels = torch.tensor( data['boolean_quality'].values ).float()\n",
        "\n",
        "print(f'Data shape: {data_t.shape}')\n",
        "print(f'Labels shape: {labels.shape}')\n",
        "\n",
        "# Labels need to be multidimentional for PyTorch (i.e. a matrix), not an array, and need to be long integers too\n",
        "labels = labels[:,None]\n",
        "print(f'Proper labels shape: {labels.shape}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zgdCQ4XqBh4h"
      },
      "outputs": [],
      "source": [
        "# %% Split into train and test data\n",
        "\n",
        "# Split with scikitlearn\n",
        "train_data,test_data,train_labels,test_labels = train_test_split(data_t,labels,test_size=0.1)\n",
        "\n",
        "# Convert into PyTorch datasets\n",
        "train_data = TensorDataset(train_data,train_labels)\n",
        "test_data  = TensorDataset(test_data,test_labels)\n",
        "\n",
        "# Convert into DataLoader objects\n",
        "batch_size   = 32\n",
        "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
        "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ocC8NH5dBuv4"
      },
      "outputs": [],
      "source": [
        "# %% Model class\n",
        "\n",
        "class ANN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Layers\n",
        "        self.input  = nn.Linear(11,16)\n",
        "        self.hid1   = nn.Linear(16,32)\n",
        "        self.hid2   = nn.Linear(32,32)\n",
        "        self.output = nn.Linear(32,1)\n",
        "\n",
        "    # Forward propagation (pass raw output)\n",
        "    def forward(self,x):\n",
        "\n",
        "        x = F.relu(self.input(x))\n",
        "        x = F.relu(self.hid1(x))\n",
        "        x = F.relu(self.hid2(x))\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "2OyM2xn1CK2Z"
      },
      "outputs": [],
      "source": [
        "# %% Function to train the model\n",
        "\n",
        "# Parameters\n",
        "num_epochs = 600\n",
        "\n",
        "def train_model(model):\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    loss_fun  = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
        "\n",
        "    # Initialise losses\n",
        "    train_acc = []\n",
        "    test_acc  = []\n",
        "    losses    = []\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        batch_acc  = []\n",
        "        batch_loss = []\n",
        "\n",
        "        for X,y in train_loader:\n",
        "\n",
        "            # Forward propagation and loss\n",
        "            yHat = model(X)\n",
        "            loss = loss_fun(yHat,y)\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Batch training accuracy\n",
        "            batch_acc.append( 100*torch.mean(((yHat>0) == y).float()).item() )\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "        # Average accuracy from batch\n",
        "        train_acc.append(np.mean(batch_acc))\n",
        "        losses.append(np.mean(batch_loss))\n",
        "\n",
        "        # Test accuracy\n",
        "        model.eval()\n",
        "\n",
        "        X,y = next(iter(test_loader))\n",
        "        with torch.no_grad():\n",
        "            yHat = model(X)\n",
        "        test_acc.append( 100*torch.mean(((yHat>0) == y).float()).item() )\n",
        "\n",
        "        model.train()\n",
        "\n",
        "    # Function output\n",
        "    return train_acc,test_acc,losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DxE_jOsjGWzW"
      },
      "outputs": [],
      "source": [
        "# %% Functions for 1D smoothing filter\n",
        "\n",
        "# Improved for edge effects - adaptive window\n",
        "def smooth_adaptive(x,k):\n",
        "    smoothed = np.zeros_like(x)\n",
        "    half_k   = k // 2\n",
        "\n",
        "    for i in range(len(x)):\n",
        "        start       = max(0, i-half_k)\n",
        "        end         = min(len(x), i+half_k + 1)\n",
        "        smoothed[i] = np.mean(x[start:end])\n",
        "\n",
        "    return smoothed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "ogXTQ_q2DQEf"
      },
      "outputs": [],
      "source": [
        "# %% Fit the model once to test\n",
        "\n",
        "# Fresh model instance\n",
        "ANN_Xavier = ANN()\n",
        "\n",
        "# Change the weights\n",
        "for p in ANN_Xavier.named_parameters():\n",
        "    if 'weight' in p[0]:\n",
        "        nn.init.xavier_normal_(p[1].data)\n",
        "\n",
        "# Fit model\n",
        "train_acc_xavier,test_acc_xavier,losses_xavier = train_model(ANN_Xavier)\n",
        "\n",
        "# Fresh model instance\n",
        "ANN_Kaiming = ANN()\n",
        "\n",
        "# Change the weights\n",
        "for p in ANN_Kaiming.named_parameters():\n",
        "    if 'weight' in p[0]:\n",
        "        nn.init.kaiming_uniform_(p[1].data,nonlinearity='relu')\n",
        "\n",
        "# Fit model\n",
        "train_acc_kaiming,test_acc_kaiming,losses_kaiming = train_model(ANN_Kaiming)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "rKqYf7XoDQB4",
        "outputId": "987a558c-9fb7-488d-8c90-01ef88ada80d"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,ax = plt.subplots(1,3,figsize=(2*phi*6,6))\n",
        "\n",
        "ax[0].plot(smooth_adaptive(losses_xavier,20),label='Xavier')\n",
        "ax[0].plot(smooth_adaptive(losses_kaiming,20),label='Kaiming')\n",
        "ax[0].set_title('Loss')\n",
        "\n",
        "ax[1].plot(smooth_adaptive(train_acc_xavier,20),label='Xavier')\n",
        "ax[1].plot(smooth_adaptive(train_acc_kaiming,20),label='Kaiming')\n",
        "ax[1].set_ylabel('Accuracy (%)')\n",
        "ax[1].set_title('Train')\n",
        "\n",
        "ax[2].plot(smooth_adaptive(test_acc_xavier,20),label='Xavier')\n",
        "ax[2].plot(smooth_adaptive(test_acc_kaiming,20),label='Kaiming')\n",
        "ax[2].set_ylabel('Accuracy (%)')\n",
        "ax[2].set_title('Test')\n",
        "\n",
        "for i in range(3):\n",
        "    ax[i].legend()\n",
        "    ax[i].grid('on')\n",
        "    ax[i].set_xlabel('Epochs')\n",
        "\n",
        "plt.savefig('figure30_code_challenge_22.png')\n",
        "plt.show()\n",
        "files.download('figure30_code_challenge_22.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "usfr71lGDP_m"
      },
      "outputs": [],
      "source": [
        "# %% Parametric experiment on weight initialisations strategies\n",
        "\n",
        "# Parameters and preallocation\n",
        "reps = 10\n",
        "\n",
        "train_acc_xavier  = np.zeros((reps))\n",
        "train_acc_kaiming = np.zeros((reps))\n",
        "test_acc_xavier   = np.zeros((reps))\n",
        "test_acc_kaiming  = np.zeros((reps))\n",
        "losses_xavier     = np.zeros((reps))\n",
        "losses_kaiming    = np.zeros((reps))\n",
        "\n",
        "# Run experiment (takes ~13 mins)\n",
        "for i in range(reps):\n",
        "\n",
        "    # Xavier\n",
        "    ANN_Xavier = ANN()\n",
        "\n",
        "    for p in ANN_Xavier.named_parameters():\n",
        "        if 'weight' in p[0]:\n",
        "            nn.init.xavier_normal_(p[1].data)\n",
        "\n",
        "    train_acc,test_acc,losses = train_model(ANN_Xavier)\n",
        "    train_acc_xavier[i] = torch.mean(torch.tensor(train_acc[-5:])).item()\n",
        "    test_acc_xavier[i]  = torch.mean(torch.tensor(test_acc[-5:])).item()\n",
        "    losses_xavier[i]    = torch.mean(torch.tensor(losses[-5:])).item()\n",
        "\n",
        "    # Kaiming\n",
        "    ANN_Kaiming = ANN()\n",
        "\n",
        "    for p in ANN_Kaiming.named_parameters():\n",
        "        if 'weight' in p[0]:\n",
        "            nn.init.kaiming_uniform_(p[1].data,nonlinearity='relu')\n",
        "\n",
        "    train_acc,test_acc,losses = train_model(ANN_Kaiming)\n",
        "    train_acc_kaiming[i] = torch.mean(torch.tensor(train_acc[-5:])).item()\n",
        "    test_acc_kaiming[i]  = torch.mean(torch.tensor(test_acc[-5:])).item()\n",
        "    losses_kaiming[i]    = torch.mean(torch.tensor(losses[-5:])).item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "dROyiEpgDP9S",
        "outputId": "1234df34-f46b-4709-df5e-991f4ec7db0b"
      },
      "outputs": [],
      "source": [
        "# %% Run t-tests\n",
        "\n",
        "# T-tests\n",
        "t_train_acc,p_train_acc = stats.ttest_ind(train_acc_xavier,train_acc_kaiming)\n",
        "t_test_acc,p_test_acc   = stats.ttest_ind(test_acc_xavier,test_acc_kaiming)\n",
        "t_losses,p_losses       = stats.ttest_ind(losses_xavier,losses_kaiming)\n",
        "\n",
        "# plot\n",
        "phi    = (1 + np.sqrt(5)) / 2\n",
        "fig,ax = plt.subplots(1,3,figsize=(2*phi*6,6))\n",
        "\n",
        "ax[0].plot(np.zeros(reps),losses_xavier,'bo',alpha=0.9)\n",
        "ax[0].plot(np.ones(reps),losses_kaiming,'ro',alpha=0.9)\n",
        "ax[0].set_title(f'Loss (t={t_losses:.2f}, p={p_losses:.3f})')\n",
        "\n",
        "ax[1].plot(np.zeros(reps),train_acc_xavier,'bo',alpha=0.9)\n",
        "ax[1].plot(np.ones(reps),train_acc_kaiming,'ro',alpha=0.9)\n",
        "ax[1].set_title(f'Train acc. (t={t_train_acc:.2f}, p={p_train_acc:.3f})')\n",
        "\n",
        "ax[2].plot(np.zeros(reps),test_acc_xavier,'bo',alpha=0.9)\n",
        "ax[2].plot(np.ones(reps),test_acc_kaiming,'ro',alpha=0.9)\n",
        "ax[2].set_title(f'Test acc. (t={t_test_acc:.2f}, p={p_test_acc:.3f})')\n",
        "\n",
        "for i in range(3):\n",
        "\n",
        "    ax[i].set_xlim([-1,2])\n",
        "    ax[i].set_xticks([0,1])\n",
        "    ax[i].set_xticklabels(['Xavier','Kaiming'])\n",
        "\n",
        "plt.savefig('figure31_code_challenge_22.png')\n",
        "plt.show()\n",
        "files.download('figure31_code_challenge_22.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYufOCc5DP6q"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 1\n",
        "#    Adam usually works better than SGD with fewer training epochs. Does Adam also equalize the differences attributable\n",
        "#    to weight initialization?\n",
        "\n",
        "# Adam does appear to equalise the differences, training accuracy is hitting a\n",
        "# ceiling effect for both init methods, and also test accuracy is not\n",
        "# significantly different\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "9qWO90CSDP4O"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 2\n",
        "#    The discrepancy between training and test performance suggests that Kaiming initialization involved some overfitting.\n",
        "#    What are some strategies you could employ to reduce overfitting?\n",
        "\n",
        "# Using SGD again for comparability. Many options are available, such as dropout\n",
        "# regularisation, or L1/L2 regularisation. Here, as an example, we can use L2\n",
        "# with a decay of 0.01\n",
        "\n",
        "# L2 seems to work a bit, as far as we can see from just one run\n",
        "\n",
        "# %% Function to train the model\n",
        "num_epochs = 600\n",
        "\n",
        "def train_model(model):\n",
        "\n",
        "    loss_fun  = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(),lr=0.01,weight_decay=0.01)\n",
        "\n",
        "    train_acc = []\n",
        "    test_acc  = []\n",
        "    losses    = []\n",
        "\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        batch_acc  = []\n",
        "        batch_loss = []\n",
        "\n",
        "        for X,y in train_loader:\n",
        "\n",
        "            yHat = model(X)\n",
        "            loss = loss_fun(yHat,y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_acc.append( 100*torch.mean(((yHat>0) == y).float()).item() )\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "        train_acc.append(np.mean(batch_acc))\n",
        "        losses.append(np.mean(batch_loss))\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        X,y = next(iter(test_loader))\n",
        "        with torch.no_grad():\n",
        "            yHat = model(X)\n",
        "        test_acc.append( 100*torch.mean(((yHat>0) == y).float()).item() )\n",
        "\n",
        "        model.train()\n",
        "\n",
        "    return train_acc,test_acc,losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "tW8QewyYMJiQ"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 3\n",
        "#    The difference between X and K initialization is likely to increase with more weights. Change the number of units in\n",
        "#    the hidden layers from 32 to 64.\n",
        "\n",
        "# Also using SGD again for comparability. Indeed, the difference increases for\n",
        "# the training sets, but on the test data there is no significant difference.\n",
        "\n",
        "# %% Model class\n",
        "class ANN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input  = nn.Linear(11,64)\n",
        "        self.hid1   = nn.Linear(64,64)\n",
        "        self.hid2   = nn.Linear(64,64)\n",
        "        self.output = nn.Linear(64,1)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        x = F.relu(self.input(x))\n",
        "        x = F.relu(self.hid1(x))\n",
        "        x = F.relu(self.hid2(x))\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
