{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXpeehXZypvO"
      },
      "outputs": [],
      "source": [
        "# %% Deep learning - Section 15.143\n",
        "#    A surprising demo of weight initialisations\n",
        "\n",
        "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
        "#   > https://www.udemy.com/course/deeplearning_x\n",
        "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
        "# from code developed by the course instructor (Mike X. Cohen), while the\n",
        "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
        "# creative input from my side. If you are interested in DL (and if you are\n",
        "# reading this statement, chances are that you are), go check out the course, it\n",
        "# is singularly good.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qOF67jQJzQGb"
      },
      "outputs": [],
      "source": [
        "# %% Libraries and modules\n",
        "import numpy               as np\n",
        "import matplotlib.pyplot   as plt\n",
        "import torch\n",
        "import torch.nn            as nn\n",
        "import seaborn             as sns\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "import pandas              as pd\n",
        "import scipy.stats         as stats\n",
        "import sklearn.metrics     as skm\n",
        "import time\n",
        "\n",
        "from torch.utils.data                 import DataLoader,TensorDataset\n",
        "from sklearn.model_selection          import train_test_split\n",
        "from google.colab                     import files\n",
        "from torchsummary                     import summary\n",
        "from IPython                          import display\n",
        "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg')\n",
        "plt.style.use('default')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p2NS_KjlGVOR"
      },
      "outputs": [],
      "source": [
        "# %% Data\n",
        "\n",
        "# Load data\n",
        "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
        "\n",
        "# Split labels from data\n",
        "labels = data[:,0]\n",
        "data   = data[:,1:]\n",
        "\n",
        "# Normalise data (original range is (0,255))\n",
        "data_norm = data / np.max(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WHCN8Ql9HkbA"
      },
      "outputs": [],
      "source": [
        "# %% Create train and test datasets\n",
        "\n",
        "# Convert to tensor (float and integers)\n",
        "data_tensor   = torch.tensor(data_norm).float()\n",
        "labels_tensor = torch.tensor(labels).long()\n",
        "\n",
        "# Split data with scikitlearn (10% test data)\n",
        "train_data,test_data,train_labels,test_labels = train_test_split(data_tensor,labels_tensor,test_size=0.1)\n",
        "\n",
        "# Convert to PyTorch datasets\n",
        "train_data = TensorDataset(train_data,train_labels)\n",
        "test_data  = TensorDataset(test_data,test_labels)\n",
        "\n",
        "# Convert into DataLoader objects\n",
        "batch_size   = 32\n",
        "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
        "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ABwKA8hrHrvY"
      },
      "outputs": [],
      "source": [
        "# %% Model class\n",
        "\n",
        "def gen_model():\n",
        "\n",
        "    class mnist_FFN(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            # Architecture\n",
        "            self.input   = nn.Linear(784,64)\n",
        "            self.hidden1 = nn.Linear(64,32)\n",
        "            self.hidden2 = nn.Linear(32,32)\n",
        "            self.output  = nn.Linear(32,10)\n",
        "\n",
        "        # Forward propagation\n",
        "        def forward(self,x):\n",
        "\n",
        "            x = F.relu(self.input(x))\n",
        "            x = F.relu(self.hidden1(x))\n",
        "            x = F.relu(self.hidden2(x))\n",
        "            x = self.output(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "\n",
        "    # Generate model instance\n",
        "    ANN = mnist_FFN()\n",
        "\n",
        "    # Loss function\n",
        "    loss_fun = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer (use Adam for optimal optimisation)\n",
        "    optimizer = torch.optim.Adam(ANN.parameters(),lr=0.01)\n",
        "\n",
        "    return ANN,loss_fun,optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJHEW4R_Jikc",
        "outputId": "b1d910ba-dff3-433a-9bb8-d4d73aae7c46"
      },
      "outputs": [],
      "source": [
        "# %% Explore weights\n",
        "\n",
        "temp_net = gen_model()[0]\n",
        "\n",
        "print('\\nHave a look at the weights from hidden layer 1 (note the fresh randomisation at each iteration):\\n')\n",
        "print(temp_net.hidden1.weight.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ydUcjo34KKOq"
      },
      "outputs": [],
      "source": [
        "# %% Function to train the model\n",
        "\n",
        "def train_model(ANN,loss_fun,optimizer):\n",
        "\n",
        "    # Parameters, inizialise vars\n",
        "    num_epochs = 10\n",
        "\n",
        "    losses    = []\n",
        "    train_acc = []\n",
        "    test_acc  = []\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Loop over training batches\n",
        "        batch_acc  = []\n",
        "        batch_loss = []\n",
        "\n",
        "        for X,y in train_loader:\n",
        "\n",
        "            # Forward propagation and loss\n",
        "            yHat = ANN(X)\n",
        "            loss = loss_fun(yHat,y)\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss and accuracy from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "            matches     = torch.argmax(yHat,axis=1) == y\n",
        "            matches_num = matches.float()\n",
        "            accuracy    = 100 * torch.mean(matches_num)\n",
        "            batch_acc.append(accuracy)\n",
        "\n",
        "        losses.append( np.mean(batch_loss) )\n",
        "        train_acc.append( np.mean(batch_acc) )\n",
        "\n",
        "        # Test accuracy\n",
        "        ANN.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            X,y = next(iter(test_loader))\n",
        "            yHat = ANN(X)\n",
        "            test_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
        "\n",
        "        ANN.train()\n",
        "\n",
        "    return train_acc,test_acc,losses,ANN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "y-vHzuHvLog4"
      },
      "outputs": [],
      "source": [
        "# %% Functions for 1D smoothing filter\n",
        "\n",
        "# Improved for edge effects - adaptive window\n",
        "def smooth_adaptive(x,k):\n",
        "    smoothed = np.zeros_like(x)\n",
        "    half_k   = k // 2\n",
        "\n",
        "    for i in range(len(x)):\n",
        "        start       = max(0, i-half_k)\n",
        "        end         = min(len(x), i+half_k + 1)\n",
        "        smoothed[i] = np.mean(x[start:end])\n",
        "\n",
        "    return smoothed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fXm8UjyjKKLN"
      },
      "outputs": [],
      "source": [
        "# %% Run the model with the default PyTorch settings\n",
        "\n",
        "ANN,loss_fun,optimizer = gen_model()\n",
        "train_acc,test_acc,losses,ANN = train_model(ANN,loss_fun,optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "XhYnPGjkKKJW",
        "outputId": "a7441a75-8722-4cb5-cc22-24c234481cca"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,ax = plt.subplots(1,figsize=(phi*6,6))\n",
        "\n",
        "ax.plot(smooth_adaptive(train_acc,2),label='Train accuracy')\n",
        "ax.plot(smooth_adaptive(test_acc,2),label='Test accuracy')\n",
        "\n",
        "ax.set_title(\"Training and test set accuracy\\n(standard weight initialisation)\")\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "ax.set_ylabel(\"Accuracy (%)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('figure1_weight_init_demo.png')\n",
        "plt.show()\n",
        "files.download('figure1_weight_init_demo.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMnoEU7oKKHK",
        "outputId": "03288b3e-a4d4-4059-e017-61b14e0876d3"
      },
      "outputs": [],
      "source": [
        "# %% Set all the weights of (only) hidden layer 1 to zero\n",
        "\n",
        "# Fresh model instance\n",
        "ANN_zero,loss_fun,optimizer = gen_model()\n",
        "\n",
        "# Set weights to zero\n",
        "ANN_zero.hidden1.weight.data = torch.zeros_like(ANN_zero.hidden1.data)\n",
        "print(ANN_zero.hidden1.weight.data)\n",
        "\n",
        "# Run the model with one layer initialised to zero\n",
        "train_acc_z,test_acc_z,losses_z,ANN_zero = train_model(ANN_zero,loss_fun,optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "u5xxAC4wKJpG",
        "outputId": "240bcf52-ba28-4838-e8d5-0fae46c5c732"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,ax = plt.subplots(1,figsize=(phi*6,6))\n",
        "\n",
        "ax.plot(range(len(train_acc)),smooth_adaptive(train_acc,2),'-',color='tab:blue')\n",
        "ax.plot(range(len(train_acc)),smooth_adaptive(test_acc,2),':',color='tab:blue')\n",
        "\n",
        "ax.plot(range(len(train_acc_z)),smooth_adaptive(train_acc_z,2),'-',color='tab:orange')\n",
        "ax.plot(range(len(train_acc_z)),smooth_adaptive(test_acc_z,2),':',color='tab:orange')\n",
        "\n",
        "ax.legend(['Train default','Test default','Train hid_1=zero','Test hid_1=zero'])\n",
        "ax.set_title(\"Training and test set accuracy\\n(standard weight initialisation vs. one layer set to zero)\")\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "ax.set_ylabel(\"Accuracy (%)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('figure2_weight_init_demo.png')\n",
        "plt.show()\n",
        "files.download('figure2_weight_init_demo.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "MtNuW5W3KJlo",
        "outputId": "9c1cbf53-8707-4ad5-c4fa-eb4716a81323"
      },
      "outputs": [],
      "source": [
        "# %% Check the weights values after training\n",
        "\n",
        "print('Weights values after training:\\n')\n",
        "print(ANN_zero.hidden1.weight.data)\n",
        "\n",
        "# Plotting\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,ax = plt.subplots(1,figsize=(phi*6,6))\n",
        "\n",
        "y,x = np.histogram(ANN.hidden1.weight.data.flatten(),30)\n",
        "ax.plot((x[1:]+x[:-1])/2,y,color='tab:blue',label='Deafult weights')\n",
        "\n",
        "y,x = np.histogram(ANN_zero.hidden1.weight.data.flatten(),30)\n",
        "ax.plot((x[1:]+x[:-1])/2,y,color='tab:red',label='hid_1=zero')\n",
        "\n",
        "ax.legend()\n",
        "ax.set_xlim(-1.5,1.5)\n",
        "\n",
        "ax.set_title(\"Histogram of weights values after training\")\n",
        "ax.set_xlabel(\"Weight value\")\n",
        "ax.set_ylabel(\"Count\")\n",
        "\n",
        "plt.savefig('figure3_weight_init_demo.png')\n",
        "plt.show()\n",
        "files.download('figure3_weight_init_demo.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "_5e1W6fDKJjM",
        "outputId": "9ed1a540-8344-43f3-9954-9609d0bdef63"
      },
      "outputs": [],
      "source": [
        "# %% Set all the weights of all the layers to zero\n",
        "\n",
        "# Fresh model instance\n",
        "ANN_all_zero,loss_fun,optimizer = gen_model()\n",
        "\n",
        "# Set weights to zero\n",
        "for p in ANN_all_zero.named_parameters():\n",
        "    p[1].data = torch.zeros_like(p[1].data)\n",
        "\n",
        "# Plot a few select parameters  to confirm (y-axis offset for visibility)\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig = plt.figure(figsize=(phi*6,6))\n",
        "\n",
        "plt.plot(0+ANN_all_zero.hidden1.weight.data.flatten(),'x',color='tab:blue')\n",
        "plt.plot(1+ANN_all_zero.hidden2.weight.data.flatten(),'x',color='tab:orange')\n",
        "plt.plot(2+ANN_all_zero.hidden1.bias.data.flatten(),'x',color='tab:green')\n",
        "\n",
        "plt.legend(['hidden1.weight','hidden2.weight','hidden1.bias'])\n",
        "plt.xlabel('Parameter index')\n",
        "plt.ylim([-1,3])\n",
        "plt.ylabel(\"Parameter value (shifted for viz.)\")\n",
        "plt.title(\"Model's parameters set to zero\")\n",
        "\n",
        "plt.savefig('figure4_weight_init_demo.png')\n",
        "plt.show()\n",
        "files.download('figure4_weight_init_demo.png')\n",
        "\n",
        "# Run the model with all layers initialised to zero\n",
        "train_acc_all_z,test_acc_all_z,losses_all_z,ANN_all_zero = train_model(ANN_all_zero,loss_fun,optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "wXW46BX5Vi57",
        "outputId": "1f23c6d0-8f0f-4250-cc3b-c1f345deb9be"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,ax = plt.subplots(1,figsize=(phi*6,6))\n",
        "\n",
        "ax.plot(range(len(train_acc)),smooth_adaptive(train_acc,2),'-',color='tab:blue')\n",
        "ax.plot(range(len(train_acc)),smooth_adaptive(test_acc,2),':',color='tab:blue')\n",
        "\n",
        "ax.plot(range(len(train_acc_all_z)),smooth_adaptive(train_acc_all_z,2),'-',color='tab:orange')\n",
        "ax.plot(range(len(test_acc_all_z)),smooth_adaptive(test_acc_all_z,2),':',color='tab:orange')\n",
        "\n",
        "ax.legend(['Train default','Test default','Train all zero','Test all zero'])\n",
        "ax.set_title(\"Training and test set accuracy\\n(standard weight initialisation vs. all layers set to zero)\")\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "ax.set_ylabel(\"Accuracy (%)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('figure5_weight_init_demo.png')\n",
        "plt.show()\n",
        "files.download('figure5_weight_init_demo.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "XW2qubMHVi4M",
        "outputId": "befb5dc9-8ee1-4069-bb0c-2dfdb374af23"
      },
      "outputs": [],
      "source": [
        "# %% Check the weights values after training\n",
        "\n",
        "# Plotting\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,ax = plt.subplots(1,figsize=(phi*6,6))\n",
        "\n",
        "y,x = np.histogram(ANN.hidden1.weight.data.flatten(),30)\n",
        "ax.plot((x[1:]+x[:-1])/2,y,color='tab:blue',label='Deafult weights')\n",
        "\n",
        "y,x = np.histogram(ANN_all_zero.hidden1.weight.data.flatten(),30)\n",
        "ax.plot((x[1:]+x[:-1])/2,y,color='tab:red',label='hid_1=zero')\n",
        "\n",
        "ax.legend()\n",
        "ax.set_xlim(-1.5,1.5)\n",
        "\n",
        "ax.set_title(\"Histogram of weights values after training\")\n",
        "ax.set_xlabel(\"Weight value\")\n",
        "ax.set_ylabel(\"Count\")\n",
        "\n",
        "plt.savefig('figure6_weight_init_demo.png')\n",
        "plt.show()\n",
        "files.download('figure6_weight_init_demo.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "mBsJDYZzVix3"
      },
      "outputs": [],
      "source": [
        "# %% Maybe it's because they are all zeros? What about another constant?\n",
        "\n",
        "# Fresh model instance\n",
        "ANN_all_one,loss_fun,optimizer = gen_model()\n",
        "\n",
        "# Set all weights to one\n",
        "for p in ANN_all_one.named_parameters():\n",
        "    p[1].data = torch.zeros_like(p[1].data) + 1\n",
        "\n",
        "# Run the model with all layers initialised to one\n",
        "train_acc_all_one,test_acc_all_one,losses_all_one,ANN_all_one = train_model(ANN_all_one,loss_fun,optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "zxqItTaQX1Hr",
        "outputId": "98ff9cdb-caf8-4ecd-8e47-b7c26d73a717"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,ax = plt.subplots(1,figsize=(phi*6,6))\n",
        "\n",
        "ax.plot(range(len(train_acc)),smooth_adaptive(train_acc,2),'-',color='tab:blue')\n",
        "ax.plot(range(len(train_acc)),smooth_adaptive(test_acc,2),':',color='tab:blue')\n",
        "\n",
        "ax.plot(range(len(train_acc_all_one)),smooth_adaptive(train_acc_all_one,2),'-',color='tab:orange')\n",
        "ax.plot(range(len(test_acc_all_one)),smooth_adaptive(test_acc_all_one,2),':',color='tab:orange')\n",
        "\n",
        "ax.legend(['Train default','Test default','Train all zero','Test all zero'])\n",
        "ax.set_title(\"Training and test set accuracy\\n(standard weight initialisation vs. all layers set to one)\")\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "ax.set_ylabel(\"Accuracy (%)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('figure7_weight_init_demo.png')\n",
        "plt.show()\n",
        "files.download('figure7_weight_init_demo.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "v5JPZOLmYLe-",
        "outputId": "3872881e-c4e7-48e8-ea09-6745c27b74b5"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 1\n",
        "#    You saw that initializing only the weights in fc1 still allowed for good performance, while having all weights be\n",
        "#    the same value led to HORRIBLE performance. Try setting all weights to ones and all biases to zeros (across all\n",
        "#    layers). Does that allow for learning? If so, how does it compare to the baseline model performance?\n",
        "\n",
        "# Fresh model instance\n",
        "ANN_extra1,loss_fun,optimizer = gen_model()\n",
        "\n",
        "# Set all weights to one and biases to 0\n",
        "for name, param in ANN_extra1.named_parameters():\n",
        "    if \"weight\" in name:\n",
        "        param.data.fill_(1.0)\n",
        "    elif \"bias\" in name:\n",
        "        param.data.zero_()\n",
        "\n",
        "# Plot\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig = plt.figure(figsize=(phi*6,6))\n",
        "\n",
        "plt.plot(0.1+ANN_extra1.hidden1.weight.data.flatten(),'x',color='tab:blue')\n",
        "plt.plot(-0.1+ANN_extra1.hidden2.weight.data.flatten(),'x',color='tab:orange')\n",
        "plt.plot(0.0+ANN_extra1.hidden1.bias.data.flatten(),'x',color='tab:green')\n",
        "\n",
        "plt.legend(['hidden1.weight','hidden2.weight','hidden1.bias'])\n",
        "plt.xlabel('Parameter index')\n",
        "plt.ylim([-1,3])\n",
        "plt.ylabel(\"Parameter value (shifted for viz.)\")\n",
        "plt.title(\"Model's parameters set to zero or one\")\n",
        "\n",
        "plt.savefig('figure8_weight_init_demo_extra1.png')\n",
        "plt.show()\n",
        "files.download('figure8_weight_init_demo_extra1.png')\n",
        "\n",
        "# Run the model with all layers initialised to one\n",
        "train_acc_extra1,test_acc_extra1,losses_extra1,ANN_extra1 = train_model(ANN_extra1,loss_fun,optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "MTX5qyVucXoZ",
        "outputId": "829b9217-aa21-4b58-babe-c984063c8b35"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 1\n",
        "#    Continue ...\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,ax = plt.subplots(1,figsize=(phi*6,6))\n",
        "\n",
        "ax.plot(range(len(train_acc)),smooth_adaptive(train_acc,2),'-',color='tab:blue')\n",
        "ax.plot(range(len(train_acc)),smooth_adaptive(test_acc,2),':',color='tab:blue')\n",
        "\n",
        "ax.plot(range(len(train_acc_extra1)),smooth_adaptive(train_acc_extra1,2),'-',color='tab:orange')\n",
        "ax.plot(range(len(test_acc_extra1)),smooth_adaptive(test_acc_extra1,2),':',color='tab:orange')\n",
        "\n",
        "ax.legend(['Train default','Test default','Train extra 1','Test extra 1'])\n",
        "ax.set_title(\"Training and test set accuracy\\n(standard weight initialisation vs. weight manipulation)\")\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "ax.set_ylabel(\"Accuracy (%)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('figure9_weight_init_demo_extra1.png')\n",
        "plt.show()\n",
        "files.download('figure9_weight_init_demo_extra1.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "4CwIyjRyYLEL",
        "outputId": "3ba982b5-4e1b-479c-9d51-0e386fc395d2"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 2\n",
        "#    Now try setting all the weights from all layers to zeros, but leave the bias terms with their initial random values.\n",
        "\n",
        "# Fresh model instance\n",
        "ANN_extra2,loss_fun,optimizer = gen_model()\n",
        "\n",
        "# Set all weights to one and biases to random\n",
        "for name, param in ANN_extra2.named_parameters():\n",
        "    if \"weight\" in name:\n",
        "        param.data.fill_(1.0)\n",
        "\n",
        "# Plot\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig = plt.figure(figsize=(phi*6,6))\n",
        "\n",
        "plt.plot(0.1+ANN_extra2.hidden1.weight.data.flatten(),'x',color='tab:blue')\n",
        "plt.plot(-0.1+ANN_extra2.hidden2.weight.data.flatten(),'x',color='tab:orange')\n",
        "plt.plot(0.0+ANN_extra2.hidden1.bias.data.flatten(),'x',color='tab:green')\n",
        "\n",
        "plt.legend(['hidden1.weight','hidden2.weight','hidden1.bias'])\n",
        "plt.xlabel('Parameter index')\n",
        "plt.ylim([-1,3])\n",
        "plt.ylabel(\"Parameter value (shifted for viz.)\")\n",
        "plt.title(\"Model's parameters set to zero or one\")\n",
        "\n",
        "plt.savefig('figure10_weight_init_demo_extra2.png')\n",
        "plt.show()\n",
        "files.download('figure10_weight_init_demo_extra2.png')\n",
        "\n",
        "# Run the model with all layers initialised to one\n",
        "train_acc_extra2,test_acc_extra2,losses_extra2,ANN_extra2 = train_model(ANN_extra2,loss_fun,optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "gOUcoSYidD0r",
        "outputId": "701dee0a-62a5-4b12-f6b0-49c5cdb9066c"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 2\n",
        "#    Continue ...\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,ax = plt.subplots(1,figsize=(phi*6,6))\n",
        "\n",
        "ax.plot(range(len(train_acc)),smooth_adaptive(train_acc,2),'-',color='tab:blue')\n",
        "ax.plot(range(len(train_acc)),smooth_adaptive(test_acc,2),':',color='tab:blue')\n",
        "\n",
        "ax.plot(range(len(train_acc_extra2)),smooth_adaptive(train_acc_extra2,2),'-',color='tab:orange')\n",
        "ax.plot(range(len(test_acc_extra2)),smooth_adaptive(test_acc_extra2,2),':',color='tab:orange')\n",
        "\n",
        "ax.legend(['Train default','Test default','Train extra 2','Test extra 2'])\n",
        "ax.set_title(\"Training and test set accuracy\\n(standard weight initialisation vs. weight manipulation)\")\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "ax.set_ylabel(\"Accuracy (%)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('figure11_weight_init_demo_extra2.png')\n",
        "plt.show()\n",
        "files.download('figure11_weight_init_demo_extra2.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "RB0TPVPKYLBl",
        "outputId": "ef631f19-ce4a-4920-94dd-303421c3d56e"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 3\n",
        "#    Finally, the opposite of #2: Set all bias terms to zero and leave the weights random. Make a plot of test accuracy\n",
        "#    performance for the baseline model, weights=0, and biases=0.\n",
        "\n",
        "# Fresh model instance\n",
        "ANN_extra3,loss_fun,optimizer = gen_model()\n",
        "\n",
        "# Set all weights to random and biases to 0\n",
        "for name, param in ANN_extra3.named_parameters():\n",
        "    if \"bias\" in name:\n",
        "        param.data.zero_()\n",
        "\n",
        "# Plot\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig = plt.figure(figsize=(phi*6,6))\n",
        "\n",
        "plt.plot(1.0+ANN_extra3.hidden1.weight.data.flatten(),'x',color='tab:blue')\n",
        "plt.plot(2.0+ANN_extra3.hidden2.weight.data.flatten(),'x',color='tab:orange')\n",
        "plt.plot(0.0+ANN_extra3.hidden1.bias.data.flatten(),'x',color='tab:green')\n",
        "\n",
        "plt.legend(['hidden1.weight','hidden2.weight','hidden1.bias'])\n",
        "plt.xlabel('Parameter index')\n",
        "plt.ylim([-1,3])\n",
        "plt.ylabel(\"Parameter value (shifted for viz.)\")\n",
        "plt.title(\"Model's parameters set to zero or one\")\n",
        "\n",
        "plt.savefig('figure12_weight_init_demo_extra3.png')\n",
        "plt.show()\n",
        "files.download('figure12_weight_init_demo_extra3.png')\n",
        "\n",
        "# Run the model with all layers initialised to one\n",
        "train_acc_extra3,test_acc_extra3,losses_extra3,ANN_extra3 = train_model(ANN_extra3,loss_fun,optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "PxP-wGoKeher",
        "outputId": "1237d43e-cc91-4897-8118-dd6e00e89f52"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 3\n",
        "#    Continue ...\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,ax = plt.subplots(1,figsize=(phi*6,6))\n",
        "\n",
        "ax.plot(range(len(train_acc)),smooth_adaptive(train_acc,2),'-',color='tab:blue')\n",
        "ax.plot(range(len(train_acc)),smooth_adaptive(test_acc,2),':',color='tab:blue')\n",
        "\n",
        "ax.plot(range(len(train_acc_extra2)),smooth_adaptive(train_acc_extra2,2),'-',color='tab:orange')\n",
        "ax.plot(range(len(test_acc_extra2)),smooth_adaptive(test_acc_extra2,2),':',color='tab:orange')\n",
        "\n",
        "ax.plot(range(len(train_acc_extra3)),smooth_adaptive(train_acc_extra3,2),'-',color='tab:red')\n",
        "ax.plot(range(len(test_acc_extra3)),smooth_adaptive(test_acc_extra3,2),':',color='tab:red')\n",
        "\n",
        "ax.legend(['Train default','Test default','Train extra 2','Test extra 2','Train extra 3','Test extra 3'])\n",
        "ax.set_title(\"Training and test set accuracy\\n(standard weight initialisation vs. weight manipulation)\")\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "ax.set_ylabel(\"Accuracy (%)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('figure13_weight_init_demo_extra3.png')\n",
        "plt.show()\n",
        "files.download('figure13_weight_init_demo_extra3.png')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
