{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXpeehXZypvO"
      },
      "outputs": [],
      "source": [
        "# %% Deep learning - Section 15.146\n",
        "#    Xavier and Kaiming initialisations\n",
        "\n",
        "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
        "#   > https://www.udemy.com/course/deeplearning_x\n",
        "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
        "# from code developed by the course instructor (Mike X. Cohen), while the\n",
        "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
        "# creative input from my side. If you are interested in DL (and if you are\n",
        "# reading this statement, chances are that you are), go check out the course, it\n",
        "# is singularly good.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qOF67jQJzQGb"
      },
      "outputs": [],
      "source": [
        "# %% Libraries and modules\n",
        "import numpy               as np\n",
        "import matplotlib.pyplot   as plt\n",
        "import torch\n",
        "import torch.nn            as nn\n",
        "import seaborn             as sns\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "import pandas              as pd\n",
        "import scipy.stats         as stats\n",
        "import sklearn.metrics     as skm\n",
        "import time\n",
        "\n",
        "from torch.utils.data                 import DataLoader,TensorDataset\n",
        "from sklearn.model_selection          import train_test_split\n",
        "from google.colab                     import files\n",
        "from torchsummary                     import summary\n",
        "from IPython                          import display\n",
        "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg')\n",
        "plt.style.use('default')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TiUhDD4Nl18d"
      },
      "outputs": [],
      "source": [
        "# %% Model class\n",
        "\n",
        "class FFN_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Architecture\n",
        "        self.input   = nn.Linear(100,100)\n",
        "        self.hidden1 = nn.Linear(100,100)\n",
        "        self.hidden2 = nn.Linear(100,100)\n",
        "        self.hidden3 = nn.Linear(100,100)\n",
        "        self.output  = nn.Linear(100,  2)\n",
        "\n",
        "    # Forward propagation\n",
        "    def forward(self,x):\n",
        "\n",
        "        x = F.relu(self.input(x))\n",
        "        x = F.relu(self.hidden1(x))\n",
        "        x = F.relu(self.hidden2(x))\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkH9M6mPuTEx",
        "outputId": "511acfb6-fba4-4bff-a48d-7f200903c75b"
      },
      "outputs": [],
      "source": [
        "# %% Model instance\n",
        "\n",
        "model = FFN_model()\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "ahrk2x37uTCE",
        "outputId": "6dfdcd58-f499-4335-e344-e56549fd3c9b"
      },
      "outputs": [],
      "source": [
        "# %% Explore weights\n",
        "\n",
        "# Get all weights and biases\n",
        "all_weight = np.array([])\n",
        "all_biases = np.array([])\n",
        "\n",
        "for p in model.named_parameters():\n",
        "\n",
        "    if 'bias' in p[0]:\n",
        "        all_biases = np.concatenate( (all_biases,p[1].data.numpy().flatten()),axis=0 )\n",
        "    elif 'weight' in p[0]:\n",
        "        all_weight = np.concatenate( (all_weight,p[1].data.numpy().flatten()),axis=0 )\n",
        "\n",
        "print(f'There are {len(all_biases)} bias parameters.')\n",
        "print(f'There are {len(all_weight)} weight parameters.')\n",
        "\n",
        "# Plot (hist)\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,ax = plt.subplots(1,3,figsize=(2*phi*6,6))\n",
        "\n",
        "ax[0].hist(all_biases,40,alpha=0.8)\n",
        "ax[0].set_title('Histogram of initial biases')\n",
        "\n",
        "ax[1].hist(all_weight,40,alpha=0.8)\n",
        "ax[1].set_title('Histogram of initial weights')\n",
        "\n",
        "# Plot (lines)\n",
        "y_bias,x_bias     = np.histogram(all_biases,30)\n",
        "y_weight,x_weight = np.histogram(all_weight,30)\n",
        "\n",
        "ax[2].plot((x_bias[1:]+x_bias[:-1])/2,y_bias/np.sum(y_bias),label='Bias')\n",
        "ax[2].plot((x_weight[1:]+x_weight[:-1])/2,y_weight/np.sum(y_weight),label='Weight')\n",
        "ax[2].set_title('Density estimate for both')\n",
        "ax[2].legend()\n",
        "\n",
        "for i in range(3):\n",
        "    ax[i].set_xlabel('Initial value')\n",
        "    ax[i].set_ylabel('Count')\n",
        "ax[2].set_ylabel('Probability')\n",
        "\n",
        "plt.savefig('figure20_xavier_kaiming_inits.png')\n",
        "plt.show()\n",
        "files.download('figure20_xavier_kaiming_inits.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "E0aBesHuuS_h",
        "outputId": "6fb1e946-9aea-4fb3-e294-eae58127ce44"
      },
      "outputs": [],
      "source": [
        "# %% Explore weights by layer\n",
        "\n",
        "# Plot\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,ax = plt.subplots(1,2,figsize=(2*phi*6,6))\n",
        "\n",
        "n_layers = sum(1 for m in model.modules() if isinstance(m,nn.Linear))\n",
        "cmap     = plt.cm.plasma(np.linspace(0.2,0.9,n_layers))\n",
        "i        = -1\n",
        "\n",
        "for p in model.named_parameters():\n",
        "\n",
        "    if 'weight' in p[0]:\n",
        "        i += 1\n",
        "\n",
        "    data = p[1].data.numpy().flatten()\n",
        "    y,x  = np.histogram(data,10)\n",
        "\n",
        "    if 'bias' in p[0]:\n",
        "        ax[0].plot((x[1:]+x[:-1])/2,y/np.sum(y),label='%s bias (N=%g)'%(p[0][:-5],len(data)),color=cmap[i])\n",
        "\n",
        "    elif 'weight' in p[0]:\n",
        "        ax[1].plot((x[1:]+x[:-1])/2,y/np.sum(y),label='%s weight (N=%g)'%(p[0][:-7],len(data)),color=cmap[i])\n",
        "\n",
        "ax[0].set_title('Biases per layer')\n",
        "ax[0].legend()\n",
        "ax[1].set_title('Weights per layer')\n",
        "ax[1].legend(bbox_to_anchor=(1,1),loc='upper left')\n",
        "\n",
        "plt.savefig('figure21_xavier_kaiming_inits.png')\n",
        "plt.show()\n",
        "files.download('figure21_xavier_kaiming_inits.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZbhFDa8xS-3",
        "outputId": "4ad1bd0a-c01f-4c93-b78a-391cfe87a8dc"
      },
      "outputs": [],
      "source": [
        "# Weird output for the output biases ?\n",
        "\n",
        "print(model.output.bias.data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "6YHnRO8Z1T_5"
      },
      "outputs": [],
      "source": [
        "# Check  docstring for linear layers\n",
        "\n",
        "nn.Linear?\n",
        "\n",
        "# Attributes:\n",
        "# weight: the learnable weights of the module of shape\n",
        "#     :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
        "#     initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
        "#     :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
        "# bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
        "#         If :attr:`bias` is ``True``, the values are initialized from\n",
        "#         :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
        "#         :math:`k = \\frac{1}{\\text{in\\_features}}`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRwU_dNd1T9a",
        "outputId": "b24d3a1b-db96-43f8-f8b1-c710a856df7c"
      },
      "outputs": [],
      "source": [
        "# Test whether the numbers match our prediction from the formula\n",
        "\n",
        "# Empirical bias range\n",
        "bias_range = [ torch.min(model.hidden1.bias.data).item(), torch.max(model.hidden1.bias.data).item() ]\n",
        "bias_count = len(model.hidden1.bias.data)\n",
        "\n",
        "# Theoretical expected value\n",
        "sigma = np.sqrt(1/bias_count)\n",
        "\n",
        "# Print\n",
        "print('Theoretical sigma = ' + str(sigma))\n",
        "print('Empirical range = ' + str(bias_range))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "yGfP-5UR1T65"
      },
      "outputs": [],
      "source": [
        "# %% Use a Xavier variance initialisation\n",
        "\n",
        "# Default in PyTorch is, as we have seen, Kaiming initialisation (uniform), here\n",
        "# how to change to a normal with Xavier variance parametrisation\n",
        "\n",
        "model = FFN_model()\n",
        "\n",
        "# Change the weights (leave biases as -default- Kaiming)\n",
        "for p in model.named_parameters():\n",
        "    if 'weight' in p[0]:\n",
        "        nn.init.xavier_normal_(p[1].data)\n",
        "\n",
        "# Rerun upper cells before continuing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFmNRy5BxS6M",
        "outputId": "4bbb2ba8-b931-43cf-a839-659a0aba1753"
      },
      "outputs": [],
      "source": [
        "# Test whether the numbers match our prediction from the formula\n",
        "\n",
        "# Empirical bias range\n",
        "weight_var   = torch.var(model.hidden1.weight.data.flatten()).item()\n",
        "weight_count = len(model.hidden1.weight.data)\n",
        "\n",
        "# Theoretical expected value (2*weight_count because same input as output)\n",
        "sigma_x = 2 / (weight_count + weight_count)\n",
        "\n",
        "# Print\n",
        "print('Theoretical sigma = ' + str(sigma_x))\n",
        "print('Empirical variance = ' + str(weight_var))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdOxrCXOxS3s"
      },
      "outputs": [],
      "source": [
        "# %% Note\n",
        "\n",
        "# There are several other weights initialization methods availabe in PyTorch.\n",
        "# > See https://pytorch.org/docs/stable/nn.init.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "cqmQxVQV4OJm"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 1\n",
        "#    Explore the weight initialization options using PyTorch's functions (nn.init.<method>).\n",
        "#    For example: apply Xavier-uniform, Kaiming, constant (this is what we did in the first video of this section).\n",
        "\n",
        "# Truncated normal\n",
        "model = FFN_model()\n",
        "\n",
        "# Change the weights\n",
        "for p in model.named_parameters():\n",
        "    if 'weight' in p[0]:\n",
        "        nn.init.trunc_normal_(p[1].data,mean=0,std=1,a=-2,b=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "C_s-8Vyc4OGn"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 1\n",
        "#    Continue ...\n",
        "\n",
        "# Truncated normal\n",
        "model = FFN_model()\n",
        "\n",
        "# Change the weights\n",
        "for p in model.named_parameters():\n",
        "    if 'weight' in p[0]:\n",
        "        nn.init.orthogonal_(p[1].data,gain=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "2xZwfXaQ54em"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 1\n",
        "#    Continue ...\n",
        "\n",
        "# Truncated normal\n",
        "model = FFN_model()\n",
        "\n",
        "# Change the weights\n",
        "for p in model.named_parameters():\n",
        "    if 'weight' in p[0]:\n",
        "        nn.init.sparse_(p[1].data,sparsity=0.25,std=0.01)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
