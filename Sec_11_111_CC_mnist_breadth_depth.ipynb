{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxLsu_5_Ao80"
      },
      "outputs": [],
      "source": [
        "# %% Deep learning - Section 11.111\n",
        "#    Code challenge 16: MNIST and breadth vs. depth\n",
        "\n",
        "#    1) Start from code from video 11.107, 07.055, and 07.057\n",
        "#    2) Vary systematically the number of hidden layers from 1 to 3\n",
        "#    3) Vary systematically the number of units for layers from 50 to 250 in\n",
        "#       steps of 50 (use same number of units per layer)\n",
        "#    4) Plot final accuracy from train and test, against number of hidden units,\n",
        "#       for 1, 2, or 3 hidden layers\n",
        "\n",
        "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
        "#   > https://www.udemy.com/course/deeplearning_x\n",
        "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
        "# from code developed by the course instructor (Mike X. Cohen), while the\n",
        "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
        "# creative input from my side. If you are interested in DL (and if you are\n",
        "# reading this statement, chances are that you are), go check out the course, it\n",
        "# is singularly good.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eFOW2iaUArr4"
      },
      "outputs": [],
      "source": [
        "# %% Libraries and modules\n",
        "import numpy               as np\n",
        "import matplotlib.pyplot   as plt\n",
        "import torch\n",
        "import torch.nn            as nn\n",
        "import seaborn             as sns\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "import pandas              as pd\n",
        "import scipy.stats         as stats\n",
        "import time\n",
        "\n",
        "from torch.utils.data                 import DataLoader,TensorDataset\n",
        "from sklearn.model_selection          import train_test_split\n",
        "from google.colab                     import files\n",
        "from torchsummary                     import summary\n",
        "from IPython                          import display\n",
        "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XJokslogtW1t"
      },
      "outputs": [],
      "source": [
        "# %% Data\n",
        "\n",
        "# Load data\n",
        "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
        "\n",
        "# Split labels from data\n",
        "labels = data[:,0]\n",
        "data   = data[:,1:]\n",
        "\n",
        "# Normalise data (original range is (0,255))\n",
        "data_norm = data / np.max(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "R5IiMkI-v6EI"
      },
      "outputs": [],
      "source": [
        "# %% Create train and test datasets\n",
        "\n",
        "# Convert to tensor (float and integers)\n",
        "data_tensor   = torch.tensor(data_norm).float()\n",
        "labels_tensor = torch.tensor(labels).long()\n",
        "\n",
        "# Split data with scikitlearn (10% test data)\n",
        "train_data,test_data,train_labels,test_labels = train_test_split(data_tensor,labels_tensor,test_size=0.1)\n",
        "\n",
        "# Convert to PyTorch datasets\n",
        "train_data = TensorDataset(train_data,train_labels)\n",
        "test_data  = TensorDataset(test_data,test_labels)\n",
        "\n",
        "# Convert into DataLoader objects\n",
        "batch_size   = 32\n",
        "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
        "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lvW5wmrhwciZ"
      },
      "outputs": [],
      "source": [
        "# %% Function to generate the model\n",
        "#    Flexibly loop over model depth/breadth\n",
        "\n",
        "def gen_model(nUnits,nLayers,drop_rate):\n",
        "\n",
        "    class mnist_FFN(nn.Module):\n",
        "        def __init__(self,nUnits,nLayers,dropout_rate):\n",
        "            super().__init__()\n",
        "\n",
        "            # Dictionary to store the layers\n",
        "            self.layers  = nn.ModuleDict()\n",
        "            self.nLayers = nLayers\n",
        "\n",
        "            # Dropout\n",
        "            self.dropout_rate = dropout_rate\n",
        "\n",
        "            # Architecture\n",
        "            self.layers['input'] = nn.Linear(784,nUnits)\n",
        "            for i in range(nLayers):\n",
        "                self.layers[f'hidden{i}'] = nn.Linear(nUnits,nUnits)\n",
        "            self.layers['output'] = nn.Linear(nUnits,10)\n",
        "\n",
        "        # Forward propagation\n",
        "        def forward(self,x):\n",
        "\n",
        "            x = F.relu(self.layers['input'](x))\n",
        "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "            for i in range(self.nLayers):\n",
        "                x = F.relu(self.layers[f'hidden{i}'](x))\n",
        "                x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "            x = self.layers['output'](x)\n",
        "\n",
        "            return x\n",
        "\n",
        "    # Create model instance\n",
        "    ANN = mnist_FFN(nUnits,nLayers,drop_rate)\n",
        "\n",
        "    # Loss function\n",
        "    loss_fun = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer (SGD to slow down learning for illustration purpose)\n",
        "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.01)\n",
        "\n",
        "    return ANN,loss_fun,optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEYzFSIJw0Dw",
        "outputId": "90e8dfe2-42e6-4498-962e-05b10ae046b6"
      },
      "outputs": [],
      "source": [
        "# %% Generate an instance of the model and check it\n",
        "\n",
        "nUnitsPerLayer = 10\n",
        "nLayers        = 2\n",
        "\n",
        "model, loss_fun, optimizer = gen_model(nUnitsPerLayer,nLayers,0.25)\n",
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUzs2XcQyuSP",
        "outputId": "8e51e9f8-d751-4ebe-fa4c-29c6e314e241"
      },
      "outputs": [],
      "source": [
        "# %% Run the model to check its internal consistency\n",
        "\n",
        "# Samples and dimentions\n",
        "tmpx = torch.randn(6,784)\n",
        "\n",
        "# Run the model\n",
        "y = model(tmpx)\n",
        "\n",
        "# Show the output shape and the output\n",
        "print(y.shape)\n",
        "print()\n",
        "print(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "b3X-u2Q_zA0b"
      },
      "outputs": [],
      "source": [
        "# %% Function to train the model\n",
        "\n",
        "def train_model(nUnits,nLayers,drop_rate):\n",
        "\n",
        "    # Parameters, model instance, inizialise vars\n",
        "    num_epochs = 60\n",
        "    ANN,loss_fun,optimizer = gen_model(nUnits,nLayers,drop_rate)\n",
        "\n",
        "    losses    = []\n",
        "    train_acc = []\n",
        "    test_acc  = []\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Loop over training batches\n",
        "        batch_acc  = []\n",
        "        batch_loss = []\n",
        "\n",
        "        for X,y in train_loader:\n",
        "\n",
        "            # Forward propagation and loss\n",
        "            yHat = ANN(X)\n",
        "            loss = loss_fun(yHat,y)\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss and accuracy from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "            matches     = torch.argmax(yHat,axis=1) == y\n",
        "            matches_num = matches.float()\n",
        "            accuracy    = 100 * torch.mean(matches_num)\n",
        "            batch_acc.append(accuracy)\n",
        "\n",
        "        losses.append( np.mean(batch_loss) )\n",
        "        train_acc.append( np.mean(batch_acc) )\n",
        "\n",
        "        # Test accuracy\n",
        "        ANN.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            X,y = next(iter(test_loader))\n",
        "            yHat = ANN(X)\n",
        "            test_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
        "\n",
        "        ANN.train()\n",
        "\n",
        "    return train_acc,test_acc,losses,ANN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfrRmvVE1vh5",
        "outputId": "d0788216-92a1-431d-8067-5a75844c16f1"
      },
      "outputs": [],
      "source": [
        "# %% Test the whole setting\n",
        "\n",
        "train_acc,test_acc,losses,ANN = train_model(32,2,0.25)\n",
        "\n",
        "print(train_acc[-1])\n",
        "print(test_acc[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2SfIhTF9zeIy"
      },
      "outputs": [],
      "source": [
        "# %% Parametric experiment on model depth and breadth\n",
        "#    This cell takes ~ 14 mins\n",
        "\n",
        "# Define model parameters (num of hidden layers and units per hidden layer)\n",
        "num_layers = np.linspace(1,3,3,dtype=int)\n",
        "num_units  = np.linspace(50,250,5,dtype=int)\n",
        "drop_rate  = 0.25\n",
        "\n",
        "# Preallocate output matrices\n",
        "accuracies_train = np.zeros(( len(num_units),len(num_layers) ))\n",
        "accuracies_test  = np.zeros(( len(num_units),len(num_layers) ))\n",
        "training_times   = np.zeros(( len(num_units),len(num_layers) ))\n",
        "\n",
        "# Buckle up, here's the experiment!\n",
        "for unit_i in range(len(num_units)):\n",
        "    for layer_i in range(len(num_layers)):\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Run model and store outputs\n",
        "        train_acc,test_acc,losses,ANN = train_model(num_units[unit_i],num_layers[layer_i],drop_rate)\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        # Store accuracies as average accuracy over last 5 epochs, and time\n",
        "        accuracies_train[unit_i,layer_i] = np.mean(train_acc[-5:])\n",
        "        accuracies_test[unit_i,layer_i]  = np.mean(test_acc[-5:])\n",
        "        training_times[unit_i,layer_i]   = duration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "bdmagDN54Nf3",
        "outputId": "8c9d6b21-186b-4b6e-ae75-c5fba3054552"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "phi = ( 1 + np.sqrt(5) ) / 2\n",
        "fig, axs = plt.subplots(1,2,figsize=(1.5*6*phi,6))\n",
        "\n",
        "cmaps = plt.cm.plasma(np.linspace(.2,.9,len(num_layers)))\n",
        "\n",
        "# Plot for Training Accuracy\n",
        "for i, nl in enumerate(num_layers):\n",
        "    axs[0].plot(num_units,accuracies_train[:, i],'-o',color=cmaps[i],label=f'{int(nl)} layers')\n",
        "axs[0].set_title('Training accuracy')\n",
        "axs[0].set_xlabel('Number of units')\n",
        "axs[0].set_ylabel('Accuracy (%)')\n",
        "axs[0].legend(title='Hidden layers')\n",
        "axs[0].grid(True)\n",
        "axs[0].set_ylim(94.8,100.2)\n",
        "\n",
        "# Plot for Test Accuracy\n",
        "for i, nl in enumerate(num_layers):\n",
        "    axs[1].plot(num_units,accuracies_test[:,i],'-o',color=cmaps[i],label=f'{int(nl)} layers')\n",
        "axs[1].set_title('Test accuracy')\n",
        "axs[1].set_xlabel('Number of units')\n",
        "axs[1].set_ylabel('Accuracy (%)')\n",
        "axs[1].legend(title='Hidden layers')\n",
        "axs[1].grid(True)\n",
        "axs[1].set_ylim(94.8,100.2)\n",
        "\n",
        "plt.suptitle('Model performance vs. width and depth', fontsize=14)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('figure41_code_challenge_16.png')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "files.download('figure41_code_challenge_16.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "bVATWuSi4uGr",
        "outputId": "4f0a64a1-2aad-43bb-c938-a1af0fe5ec36"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "phi = ( 1 + np.sqrt(5) ) / 2\n",
        "fig, axs = plt.subplots(1,2,figsize=(1.5*6*phi,6))\n",
        "\n",
        "cmaps = plt.cm.plasma(np.linspace(.2,.9,len(num_units)))\n",
        "\n",
        "# Plot for Training Accuracy\n",
        "for i,n_units in enumerate(num_units):\n",
        "    axs[0].plot(num_layers,accuracies_train[i,:],'-o',color=cmaps[i],label=f'{int(n_units)} units')\n",
        "axs[0].set_title('Training accuracy')\n",
        "axs[0].set_xlabel('Number of hidden layers')\n",
        "axs[0].set_ylabel('Accuracy (%)')\n",
        "axs[0].legend(title='Units per layer')\n",
        "axs[0].grid(True)\n",
        "axs[0].set_ylim(94.8,100.2)\n",
        "\n",
        "# Plot for Test Accuracy\n",
        "for i,n_units in enumerate(num_units):\n",
        "    axs[1].plot(num_layers,accuracies_test[i,:],'-o',color=cmaps[i],label=f'{int(n_units)} units')\n",
        "axs[1].set_title('Test accuracy')\n",
        "axs[1].set_xlabel('Number of hidden layers')\n",
        "axs[1].set_ylabel('Accuracy (%)')\n",
        "axs[1].legend(title='Units per layer')\n",
        "axs[1].grid(True)\n",
        "axs[1].set_ylim(94.8,100.2)\n",
        "\n",
        "plt.suptitle('Model performance vs. breadth and depth',fontsize=14)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('figure42_code_challenge_16.png')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "files.download('figure42_code_challenge_16.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlegdBWB69Rs"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 1\n",
        "#    Based on the results above, pick the best and the worst model architectures.\n",
        "#    Then, run only those two models again using regularization (you can pick which\n",
        "#    regularization method to use). Does this help the bad model and/or hurt the good model?\n",
        "\n",
        "# A bit of a lazy approach here. I re-ran everything using a dropout\n",
        "# regularisation, with a value of 0.25. Interestingly, for these data set, the\n",
        "# regularisation seems to worsen the training accuracy in general (which is\n",
        "# indeed expected), but is also seems to worsen the test accuracy, especially\n",
        "# for models with fewer units; I'd guess this is can be at least partially\n",
        "# explained because with fewer units in general, a constant dropout rate is a\n",
        "# bit more brutal than for models with more units.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "ScXMQDFbGVTH",
        "outputId": "bb7eb03e-1eca-4c57-ee21-c65c458e8448"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 2\n",
        "#    Time how long each model takes to train (from the full experiment, not just the two models\n",
        "#    for exploration #1). Obviously, both factors (depth and breadth) affect training time, but\n",
        "#    which factor seems to have a bigger influence on model training time?\n",
        "\n",
        "# Results based on dropout regularisation values of 0 and 0.25. Regularisation\n",
        "# greater than 0 takes a bit more time in general, but most importantly there\n",
        "# seems to be a mild moderation by number of layers, and a stronger moderation\n",
        "# by number of units.\n",
        "\n",
        "# Plot time matrix\n",
        "phi = ( 1 + np.sqrt(5) ) / 2\n",
        "fig, ax = plt.subplots(figsize=(phi*6,6))\n",
        "im = ax.imshow(training_times,cmap='plasma',aspect='auto')\n",
        "\n",
        "ax.set_xticks(range(len(num_layers)))\n",
        "ax.set_xticklabels([int(n) for n in num_layers])\n",
        "ax.set_xlabel('Number of Layers')\n",
        "\n",
        "ax.set_yticks(range(len(num_units)))\n",
        "ax.set_yticklabels([int(n) for n in num_units])\n",
        "ax.set_ylabel('Number of Units')\n",
        "\n",
        "cbar = plt.colorbar(im,ax=ax)\n",
        "cbar.set_label('Training time (s)',rotation=270,labelpad=15)\n",
        "\n",
        "for i in range(len(num_units)):\n",
        "    for j in range(len(num_layers)):\n",
        "        value = training_times[i,j]\n",
        "        ax.text(j,i,f'{value:.0f} s',ha='center',va='center',color='white' if value < np.max(training_times)*0.6 else 'black')\n",
        "\n",
        "total_secs = np.sum(training_times)\n",
        "mins       = int(total_secs // 60)\n",
        "secs       = int(total_secs % 60)\n",
        "ax.set_title(f'Training time per model configuration (CPU)\\n(Total time = {mins}:{secs} mins)')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('figure45_code_challenge_16_extra2.png')\n",
        "plt.show()\n",
        "files.download('figure45_code_challenge_16_extra2.png')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
