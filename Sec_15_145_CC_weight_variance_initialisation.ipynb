{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUNd0bnSPkJw"
      },
      "outputs": [],
      "source": [
        "# %% Deep learning - Section 15.145\n",
        "#    Code challenge 21: weight variance inits\n",
        "#\n",
        "#    1) Start from code from video 13.144 (mnist dataset)\n",
        "#    2) Initialise the weights to be normally distributed random numbers, and\n",
        "#       vary parametrically the std between 0.001 and 10 in 25 log steps\n",
        "#    3) Plot the training accuracy averaged over the last 3 epochs per std step\n",
        "#    4) Plot an histogram of all the post-training weights (i.e. one histogram\n",
        "#       per std step)\n",
        "\n",
        "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
        "#   > https://www.udemy.com/course/deeplearning_x\n",
        "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
        "# from code developed by the course instructor (Mike X. Cohen), while the\n",
        "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
        "# creative input from my side. If you are interested in DL (and if you are\n",
        "# reading this statement, chances are that you are), go check out the course, it\n",
        "# is singularly good.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "o2_4fR9-Pyrt"
      },
      "outputs": [],
      "source": [
        "# %% Libraries and modules\n",
        "import numpy               as np\n",
        "import matplotlib.pyplot   as plt\n",
        "import torch\n",
        "import torch.nn            as nn\n",
        "import seaborn             as sns\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "import pandas              as pd\n",
        "import scipy.stats         as stats\n",
        "import sklearn.metrics     as skm\n",
        "import time\n",
        "\n",
        "from torch.utils.data                 import DataLoader,TensorDataset\n",
        "from sklearn.model_selection          import train_test_split\n",
        "from google.colab                     import files\n",
        "from torchsummary                     import summary\n",
        "from IPython                          import display\n",
        "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg')\n",
        "plt.style.use('default')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p2NS_KjlGVOR"
      },
      "outputs": [],
      "source": [
        "# %% Data\n",
        "\n",
        "# Load data\n",
        "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
        "\n",
        "# Split labels from data\n",
        "labels = data[:,0]\n",
        "data   = data[:,1:]\n",
        "\n",
        "# Normalise data (original range is (0,255))\n",
        "data_norm = data / np.max(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WHCN8Ql9HkbA"
      },
      "outputs": [],
      "source": [
        "# %% Create train and test datasets\n",
        "\n",
        "# Convert to tensor (float and integers)\n",
        "data_tensor   = torch.tensor(data_norm).float()\n",
        "labels_tensor = torch.tensor(labels).long()\n",
        "\n",
        "# Split data with scikitlearn (10% test data)\n",
        "train_data,test_data,train_labels,test_labels = train_test_split(data_tensor,labels_tensor,test_size=0.1)\n",
        "\n",
        "# Convert to PyTorch datasets\n",
        "train_data = TensorDataset(train_data,train_labels)\n",
        "test_data  = TensorDataset(test_data,test_labels)\n",
        "\n",
        "# Convert into DataLoader objects\n",
        "batch_size   = 32\n",
        "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
        "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ABwKA8hrHrvY"
      },
      "outputs": [],
      "source": [
        "# %% Model class\n",
        "\n",
        "def gen_model():\n",
        "\n",
        "    class mnist_FFN(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            # Architecture\n",
        "            self.input   = nn.Linear(784,64)\n",
        "            self.hidden1 = nn.Linear(64,32)\n",
        "            self.hidden2 = nn.Linear(32,32)\n",
        "            self.output  = nn.Linear(32,10)\n",
        "\n",
        "        # Forward propagation\n",
        "        def forward(self,x):\n",
        "\n",
        "            x = F.relu(self.input(x))\n",
        "            x = F.relu(self.hidden1(x))\n",
        "            x = F.relu(self.hidden2(x))\n",
        "            x = self.output(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "\n",
        "    # Generate model instance\n",
        "    ANN = mnist_FFN()\n",
        "\n",
        "    # Loss function\n",
        "    loss_fun = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer (use Adam for optimal optimisation)\n",
        "    optimizer = torch.optim.Adam(ANN.parameters(),lr=0.01)\n",
        "\n",
        "    return ANN,loss_fun,optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KV1XRNDxpKQD"
      },
      "outputs": [],
      "source": [
        "# %% Function to train the model\n",
        "\n",
        "def train_model(ANN,loss_fun,optimizer):\n",
        "\n",
        "    # Parameters, inizialise vars\n",
        "    num_epochs = 10\n",
        "\n",
        "    losses    = []\n",
        "    train_acc = []\n",
        "    test_acc  = []\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Loop over training batches\n",
        "        batch_acc  = []\n",
        "        batch_loss = []\n",
        "\n",
        "        for X,y in train_loader:\n",
        "\n",
        "            # Forward propagation and loss\n",
        "            yHat = ANN(X)\n",
        "            loss = loss_fun(yHat,y)\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss and accuracy from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "            matches     = torch.argmax(yHat,axis=1) == y\n",
        "            matches_num = matches.float()\n",
        "            accuracy    = 100 * torch.mean(matches_num)\n",
        "            batch_acc.append(accuracy)\n",
        "\n",
        "        losses.append( np.mean(batch_loss) )\n",
        "        train_acc.append( np.mean(batch_acc) )\n",
        "\n",
        "        # Test accuracy\n",
        "        ANN.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            X,y = next(iter(test_loader))\n",
        "            yHat = ANN(X)\n",
        "            test_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
        "\n",
        "        ANN.train()\n",
        "\n",
        "    return train_acc,test_acc,losses,ANN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jtSWoiT9pKNb"
      },
      "outputs": [],
      "source": [
        "# %% Functions for 1D smoothing filter\n",
        "\n",
        "# Improved for edge effects - adaptive window\n",
        "def smooth_adaptive(x,k):\n",
        "    smoothed = np.zeros_like(x)\n",
        "    half_k   = k // 2\n",
        "\n",
        "    for i in range(len(x)):\n",
        "        start       = max(0, i-half_k)\n",
        "        end         = min(len(x), i+half_k + 1)\n",
        "        smoothed[i] = np.mean(x[start:end])\n",
        "\n",
        "    return smoothed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "B3vppOt6pSY5"
      },
      "outputs": [],
      "source": [
        "# %% Train models with various weight variance initialisation\n",
        "\n",
        "# Parameters and preallocations\n",
        "stds             = np.logspace(np.log10(10e-5),np.log10(10),25)\n",
        "train_acc        = []\n",
        "saved_params     = []\n",
        "saved_params_pre = []\n",
        "\n",
        "# Loop over variances (takes ~8 mins)\n",
        "for std in stds:\n",
        "\n",
        "    # Fresh model instance\n",
        "    ANN,loss_fun,optimizer = gen_model()\n",
        "\n",
        "    # Set all weights to normally distributed random number\n",
        "    for name,param in ANN.named_parameters():\n",
        "        if \"weight\" in name:\n",
        "            param.data.normal_(mean=0.0,std=std)\n",
        "        elif \"bias\" in name:\n",
        "            param.data.normal_(mean=0.0,std=std)\n",
        "\n",
        "    # Save initialised weights before training\n",
        "    saved_params_pre.append(copy.deepcopy(ANN.state_dict()))\n",
        "\n",
        "    # Run the model\n",
        "    train_acc_i,test_acc_i,losses_i,ANN_i = train_model(ANN,loss_fun,optimizer)\n",
        "    train_acc.append(np.mean(train_acc_i[-3:]))\n",
        "\n",
        "    # Save initialised weights after training\n",
        "    saved_params.append(copy.deepcopy(ANN_i.state_dict()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "n8WhT5KXpSWf",
        "outputId": "c3a3f97e-47e3-4e44-85cc-ecef81501aaf"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig = plt.figure(figsize=(phi*6,6))\n",
        "\n",
        "plt.plot(stds,train_acc,'s-',color='tab:red')\n",
        "plt.xscale('log')\n",
        "plt.axhline(y=20,color='grey',linestyle=':',linewidth=0.8)\n",
        "plt.axhline(y=80,color='grey',linestyle=':',linewidth=0.8)\n",
        "\n",
        "plt.legend([\"Accuracy over models\"],loc=\"center left\")\n",
        "plt.xlabel(\"Standar deviation for weight initialisation\")\n",
        "plt.ylabel(\"Average accuracy over last 3 epochs (%)\")\n",
        "plt.title(\"Models' accuracy over inits\")\n",
        "\n",
        "plt.savefig('figure14_code_challenge_21.png')\n",
        "plt.show()\n",
        "files.download('figure14_code_challenge_21.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "tpiDmqmLpKLH",
        "outputId": "80432b5f-aebf-4c8c-9be9-3e9080f296c2"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "# Preallocate hist bin centres and count\n",
        "n_bins    = 80\n",
        "n_nodels  = len(saved_params)\n",
        "hist_data = np.zeros((n_nodels,2,n_bins))\n",
        "\n",
        "# Retrieve weights (leave out biases)\n",
        "for i,model_dict in enumerate(saved_params):\n",
        "\n",
        "    all_weights = []\n",
        "\n",
        "    for param_name, param_tensor in model_dict.items():\n",
        "        if \"weight\" in param_name:\n",
        "            all_weights.append(param_tensor.detach().numpy().flatten())\n",
        "\n",
        "    # Concatenate and compute histogram\n",
        "    all_weights      = np.concatenate(all_weights)\n",
        "    counts,bin_edges = np.histogram(all_weights,bins=n_bins,range=(-2,2),density=True)\n",
        "\n",
        "    # Bin centers\n",
        "    bin_centers = (bin_edges[1:] + bin_edges[:-1]) / 2\n",
        "\n",
        "    # Store\n",
        "    hist_data[i,0,:] = bin_centers\n",
        "    hist_data[i,1,:] = counts\n",
        "\n",
        "# Plot\n",
        "cmap = plt.cm.plasma(np.linspace(0.2,0.9,len(stds)))\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig = plt.figure(figsize=(phi*7,7))\n",
        "\n",
        "for i in range(len(stds)):\n",
        "    plt.plot(hist_data[i,0,:],hist_data[i,1,:],color=cmap[-i-1],lw=1)\n",
        "\n",
        "plt.title('Histogram of post-training weights by std initialisation')\n",
        "plt.xlabel('Weight value')\n",
        "plt.ylabel('Count (normalised)')\n",
        "plt.legend(np.round(stds,4),bbox_to_anchor=(1,1),loc='upper left')\n",
        "\n",
        "plt.savefig('figure15_code_challenge_21.png')\n",
        "plt.show()\n",
        "files.download('figure15_code_challenge_21.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "Ajpwl-ZxQVY3",
        "outputId": "158f5530-f165-44bd-88a6-fa4bc18340b1"
      },
      "outputs": [],
      "source": [
        "# %% Execise 1\n",
        "#    Are you sure we calculated the standard deviations correctly? Immediately after the code that initializes the weights,\n",
        "#    write some more code that gets all of the weights (across all layers), compute the standard deviation, and then print\n",
        "#    out the desired and actual standard deviations. Note that they won't correspond exactly, due to sampling variability.\n",
        "#    Also note that because this is just a sanity check, you don't actually need to train the model; just verify that the\n",
        "#    weights have been correctly calculated.\n",
        "\n",
        "# Compute mismatch\n",
        "actual_stds = []\n",
        "\n",
        "for model_dict in saved_params_pre:\n",
        "\n",
        "    all_weights = []\n",
        "\n",
        "    for param_name, param_tensor in model_dict.items():\n",
        "        if \"weight\" in param_name:\n",
        "            all_weights.append(param_tensor.detach().numpy().flatten())\n",
        "\n",
        "    all_weights = np.concatenate(all_weights)\n",
        "    actual_stds.append(np.std(all_weights))\n",
        "\n",
        "# Plot\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig = plt.figure(figsize=(phi*6,6))\n",
        "\n",
        "plt.plot(stds,stds,'o',label='Target std',color='tab:green',markersize=10)\n",
        "plt.plot(stds,actual_stds,'o',color='tab:red',markersize=5,label='Measured std')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Desired std for weight initialization')\n",
        "plt.ylabel('Measured std of initialized weights')\n",
        "plt.title(f'Check initialization accuracy\\n(max mismatch = {round(np.max(np.abs(stds-actual_stds)),5)})')\n",
        "plt.legend()\n",
        "\n",
        "plt.savefig('figure16_code_challenge_21_extra1.png')\n",
        "plt.show()\n",
        "files.download('figure16_code_challenge_21_extra1.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llFbAjQJPyoF",
        "outputId": "269e3145-d4e5-47be-aca4-e3329263213d"
      },
      "outputs": [],
      "source": [
        "# %% Execise 2\n",
        "#    Here we used torch.randn to assign the weights. randn creates Gaussian random numbers with a mean of 0, and thus\n",
        "#    the weights were initialized with both positive and negative values. Try running the experiment again using\n",
        "#    torch.rand, which creates uniformly distributed numbers between 0 and 1.\n",
        "#    NOTE: Specifying the standard deviation of a uniform distribution is slightly more involved compared to a normal\n",
        "#    distribution. See https://math.stackexchange.com/a/140081 for instructions.\n",
        "\n",
        "# Re-run with uniform distribution\n",
        "train_acc        = []\n",
        "saved_params     = []\n",
        "saved_params_pre = []\n",
        "\n",
        "ANN,loss_fun,optimizer = gen_model()\n",
        "\n",
        "for name,param in ANN.named_parameters():\n",
        "    if \"weight\" in name:\n",
        "        param.data.uniform_(0,1)\n",
        "    elif \"bias\" in name:\n",
        "        param.data.uniform_(0,1)\n",
        "\n",
        "saved_params_pre.append(copy.deepcopy(ANN.state_dict()))\n",
        "\n",
        "train_acc_i,test_acc_i,losses_i,ANN_i = train_model(ANN,loss_fun,optimizer)\n",
        "train_acc.append(np.mean(train_acc_i[-3:]))\n",
        "\n",
        "saved_params.append(copy.deepcopy(ANN_i.state_dict()))\n",
        "\n",
        "print(f\"Accuracy with uniform distribution = {train_acc[-1]:.5f} %\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "d52uN2m0GWRT"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 2\n",
        "#    Continue ...\n",
        "\n",
        "# Same as above (uniform) but matching the stds from the normal distributions\n",
        "\n",
        "# Note: variance for a uniform distribution from a normal std :\n",
        "#       > var_u = (b-a)^2 / 12\n",
        "#       > var_u = std^2\n",
        "# but we want a symmetrical distribution so a=b :\n",
        "#       > std^2 = (2a)^2 /12\n",
        "#       > a = sqrt(3)std\n",
        "\n",
        "# Re-run with uniform distribution (takes ~8 mins)\n",
        "stds             = np.logspace(np.log10(10e-5),np.log10(10),25)\n",
        "train_acc        = []\n",
        "saved_params     = []\n",
        "saved_params_pre = []\n",
        "\n",
        "for std in stds:\n",
        "\n",
        "    ANN,loss_fun,optimizer = gen_model()\n",
        "    a = np.sqrt(3) * std\n",
        "\n",
        "    for name,param in ANN.named_parameters():\n",
        "        if \"weight\" in name:\n",
        "            param.data.uniform_(-a,a)\n",
        "        elif \"bias\" in name:\n",
        "            param.data.uniform_(-a,a)\n",
        "\n",
        "    saved_params_pre.append(copy.deepcopy(ANN.state_dict()))\n",
        "\n",
        "    train_acc_i,test_acc_i,losses_i,ANN_i = train_model(ANN,loss_fun,optimizer)\n",
        "    train_acc.append(np.mean(train_acc_i[-3:]))\n",
        "\n",
        "    saved_params.append(copy.deepcopy(ANN_i.state_dict()))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
