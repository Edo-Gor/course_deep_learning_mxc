{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zc-76aJYXIuu"
      },
      "outputs": [],
      "source": [
        "# %% Deep learning - Section 13.133\n",
        "#    Code challenge 20: MNIST with unequal groups\n",
        "# \n",
        "#    1) Start from code from video 13.132 (mnist dataset)\n",
        "#    2) Reduce the number of 7s to 500 (random selection) and plot an histogram\n",
        "#       to confirm it works\n",
        "#    3) Inspect the digit-specific performance metrics (APRF)\n",
        "#    4) Before starting, develop an hypothesis of what you might end up seeing;\n",
        "#       here's mine: since there are fewer samples to extract pattern from, the\n",
        "#       accuracy should decrease, and the recall (i.e., sensitivity) should also\n",
        "#       be particularly affected (i.e., a bias towards not recognising 7s that\n",
        "#       are actually 7s)\n",
        "\n",
        "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
        "#   > https://www.udemy.com/course/deeplearning_x\n",
        "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
        "# from code developed by the course instructor (Mike X. Cohen), while the\n",
        "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
        "# creative input from my side. If you are interested in DL (and if you are\n",
        "# reading this statement, chances are that you are), go check out the course, it\n",
        "# is singularly good.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "90l6mgXJYlJw"
      },
      "outputs": [],
      "source": [
        "# %% Libraries and modules\n",
        "import numpy               as np\n",
        "import matplotlib.pyplot   as plt\n",
        "import torch\n",
        "import torch.nn            as nn\n",
        "import seaborn             as sns\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "import pandas              as pd\n",
        "import scipy.stats         as stats\n",
        "import sklearn.metrics     as skm\n",
        "import time\n",
        "\n",
        "from torch.utils.data                 import DataLoader,TensorDataset\n",
        "from sklearn.model_selection          import train_test_split\n",
        "from google.colab                     import files\n",
        "from torchsummary                     import summary\n",
        "from IPython                          import display\n",
        "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dt_ZVtJWYlBc"
      },
      "outputs": [],
      "source": [
        "# %% Function to get the data\n",
        "\n",
        "# Load data\n",
        "data_all = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
        "\n",
        "# Function\n",
        "def get_dataset(digits=None,n_per_digits=None):\n",
        "\n",
        "    # Remove labels (i.e., numbers IDs) from dataset\n",
        "    labels = data_all[:,0]\n",
        "    data   = data_all[:,1:]\n",
        "\n",
        "    # Normalize to [0,1]\n",
        "    data_norm = data / np.max(data)\n",
        "\n",
        "    # Covert to tensor\n",
        "    data_T   = torch.tensor(data_norm).float()\n",
        "    labels_T = torch.tensor(labels).long()\n",
        "\n",
        "    # Split data with scikitlearn\n",
        "    train_data,test_data, train_labels,test_labels = train_test_split(data_T,labels_T,test_size=0.1)\n",
        "\n",
        "    # Select subsample\n",
        "    if digits is not None and n_per_digits is not None:\n",
        "\n",
        "        keep_indices = np.zeros(labels.shape[0],dtype=bool)\n",
        "        train_labels_np = train_labels.numpy()\n",
        "\n",
        "        for lbl in np.unique(train_labels_np):\n",
        "\n",
        "            idx = np.where(train_labels_np == lbl)[0]\n",
        "\n",
        "            if lbl in digits:\n",
        "\n",
        "                idx = np.random.choice(idx,n_per_digits,replace=False)\n",
        "\n",
        "            keep_indices[idx] = True\n",
        "\n",
        "        train_data   = train_data[keep_indices]\n",
        "        train_labels = train_labels[keep_indices]\n",
        "\n",
        "    # PyTorch datasets\n",
        "    train_data = TensorDataset(train_data,train_labels)\n",
        "    test_data  = TensorDataset(test_data,test_labels)\n",
        "\n",
        "    # DataLoader objects\n",
        "    batch_size   = 32\n",
        "    train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
        "    test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n",
        "\n",
        "    return train_loader,test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "6SRrStnXjf0P",
        "outputId": "8e7026cc-218a-4eb5-c87e-7d2fc134c017"
      },
      "outputs": [],
      "source": [
        "# %% Test function\n",
        "\n",
        "# Call function\n",
        "train_loader,test_loader = get_dataset(digits=[7],n_per_digits=500)\n",
        "\n",
        "# Extract train labels\n",
        "train_labels = train_loader.dataset.tensors[1].numpy()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.hist(train_labels,bins=np.arange(train_labels.min(),train_labels.max()+2)-0.5,rwidth=0.8)\n",
        "plt.xticks(range(int(train_labels.min()),int(train_labels.max())+1))\n",
        "plt.xlabel(\"Digits\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Train label distribution\")\n",
        "\n",
        "plt.savefig('figure17_code_challenge_20.png')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "files.download('figure17_code_challenge_20.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PW0pk8FbmxoC"
      },
      "outputs": [],
      "source": [
        "# %% Model class\n",
        "\n",
        "def gen_model():\n",
        "\n",
        "    class model(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            # Architecture\n",
        "            self.input  = nn.Linear(784,64 )\n",
        "            self.hid1   = nn.Linear( 64,32 )\n",
        "            self.hid2   = nn.Linear( 32,32 )\n",
        "            self.output = nn.Linear( 32,10 )\n",
        "\n",
        "        def forward(self,x):\n",
        "            x = F.relu(self.input(x))\n",
        "            x = F.relu(self.hid1(x))\n",
        "            x = F.relu(self.hid2(x))\n",
        "\n",
        "            return self.output(x)\n",
        "\n",
        "    # Model instance, loss function, and optimizer\n",
        "    ANN       = model()\n",
        "    loss_fun  = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(ANN.parameters(),lr=0.01)\n",
        "\n",
        "    return ANN,loss_fun,optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Q7-p5EnSmxgj"
      },
      "outputs": [],
      "source": [
        "# %% Function to train the model\n",
        "\n",
        "def train_model():\n",
        "\n",
        "    # Epochs and fresh model instance\n",
        "    num_epochs = 10\n",
        "    ANN,loss_fun,optimizer = gen_model()\n",
        "\n",
        "    # Preallocate vars\n",
        "    losses    = torch.zeros(num_epochs)\n",
        "    train_acc = torch.zeros(num_epochs)\n",
        "    test_acc  = torch.zeros(num_epochs)\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_acc  = []\n",
        "        batch_loss = []\n",
        "\n",
        "        for X,y in train_loader:\n",
        "\n",
        "            # Forward pass, backpropagation, and optimizer step\n",
        "            yHat = ANN(X)\n",
        "            loss = loss_fun(yHat,y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss and accuracy from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "            batch_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
        "\n",
        "        losses[epoch_i]    = np.mean(batch_loss).item()\n",
        "        train_acc[epoch_i] = np.mean(batch_acc).item()\n",
        "\n",
        "        # Test accuracy\n",
        "        ANN.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            X,y = next(iter(test_loader))\n",
        "            yHat = ANN(X)\n",
        "            test_acc[epoch_i] = 100*torch.mean((torch.argmax(yHat,axis=1)==y).float())\n",
        "\n",
        "        ANN.train()\n",
        "\n",
        "    return train_acc,test_acc,losses,ANN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "owWebbeNmxdL"
      },
      "outputs": [],
      "source": [
        "# %% Fit the model\n",
        "\n",
        "train_loader,test_loader = get_dataset(digits=[7],n_per_digits=500)\n",
        "train_acc,test_acc,losses,ANN = train_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVEGgcFsmxai",
        "outputId": "f1cf9579-003f-4782-d2f0-2b13c354f389"
      },
      "outputs": [],
      "source": [
        "# %% Get data to compute performance measures on train and test data\n",
        "\n",
        "# Predictions for (all) training data (i.e. raw output of last layer)\n",
        "yHat        = ANN(train_loader.dataset.tensors[0])\n",
        "train_preds = torch.argmax(yHat,axis=1)\n",
        "print(train_preds.shape)\n",
        "\n",
        "# Predictions for test data (i.e. raw output of last layer)\n",
        "yHat       = ANN(test_loader.dataset.tensors[0])\n",
        "test_preds = torch.argmax(yHat,axis=1)\n",
        "print(test_preds.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "Tc7Kfm-5nZWs"
      },
      "outputs": [],
      "source": [
        "# %% Compute performance measures on train and test data\n",
        "\n",
        "# Preallocate\n",
        "train_metrics = np.zeros(4)\n",
        "test_metrics  = np.zeros(4)\n",
        "\n",
        "# Training performance measures (accuracy is already an overall measure)\n",
        "train_metrics[0] = skm.accuracy_score (train_loader.dataset.tensors[1],(train_preds).float())\n",
        "train_metrics[1] = skm.precision_score(train_loader.dataset.tensors[1],(train_preds).float(),average='weighted')\n",
        "train_metrics[2] = skm.recall_score   (train_loader.dataset.tensors[1],(train_preds).float(),average='weighted')\n",
        "train_metrics[3] = skm.f1_score       (train_loader.dataset.tensors[1],(train_preds).float(),average='weighted')\n",
        "\n",
        "# Test performance measures\n",
        "test_metrics[0] = skm.accuracy_score (test_loader.dataset.tensors[1],(test_preds).float())\n",
        "test_metrics[1] = skm.precision_score(test_loader.dataset.tensors[1],(test_preds).float(),average='weighted')\n",
        "test_metrics[2] = skm.recall_score   (test_loader.dataset.tensors[1],(test_preds).float(),average='weighted')\n",
        "test_metrics[3] = skm.f1_score       (test_loader.dataset.tensors[1],(test_preds).float(),average='weighted')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "pHv1DIDVnZTw",
        "outputId": "af1ba15c-b62f-41df-9bc2-5c92a20adcf2"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig = plt.figure(figsize=(6*phi,6))\n",
        "\n",
        "plt.bar(np.arange(4)-.1,train_metrics,.5)\n",
        "plt.bar(np.arange(4)+.1,test_metrics,.5)\n",
        "plt.xticks([0,1,2,3],['Accuracy','Precision','Recall','F1-score'])\n",
        "ymin = min(train_metrics.min(), test_metrics.min())\n",
        "ymax = max(train_metrics.max(), test_metrics.max())\n",
        "plt.ylim([ymin*0.95,ymax*1.05])\n",
        "plt.legend(['Train','Test'])\n",
        "plt.title('Performance metrics')\n",
        "\n",
        "plt.savefig('figure18_code_challenge_20.png')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "files.download('figure18_code_challenge_20.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "eJoY8OxwmxX-",
        "outputId": "0d978048-3be1-4965-a42d-c27d4a5caa86"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "y_true   = test_loader.dataset.tensors[1].numpy()\n",
        "y_pred   = test_preds\n",
        "accuracy = []\n",
        "for lbl in range(10):\n",
        "    mask    = (y_true==lbl)\n",
        "    acc_lbl = (y_pred[mask]==y_true[mask]).float().mean()\n",
        "    accuracy.append(acc_lbl)\n",
        "\n",
        "accuracy  = np.array(accuracy)\n",
        "precision = skm.precision_score(test_loader.dataset.tensors[1],test_preds,average=None)\n",
        "recall    = skm.recall_score   (test_loader.dataset.tensors[1],test_preds,average=None)\n",
        "f1_score  = skm.f1_score       (test_loader.dataset.tensors[1],test_preds,average=None)\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig = plt.figure(figsize=(1.5*6*phi,6))\n",
        "\n",
        "plt.bar(np.arange(10)-.20,accuracy,.25)\n",
        "plt.bar(np.arange(10)-.10,precision,.25)\n",
        "plt.bar(np.arange(10)+.10,recall,.25)\n",
        "plt.bar(np.arange(10)+.20,f1_score,.25)\n",
        "plt.xticks(range(10),range(10))\n",
        "ymin = min(accuracy.min(),precision.min(),recall.min(),f1_score.min())\n",
        "ymax = max(accuracy.min(),precision.max(),recall.max(),f1_score.max())\n",
        "plt.ylim([ymin*0.95,ymax*1.05])\n",
        "plt.xlabel('Digit')\n",
        "plt.legend(['Accuracy','Precision','Recall','F1'])\n",
        "plt.title('Category-specific performance metrics')\n",
        "\n",
        "plt.savefig('figure19_code_challenge_20.png')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "files.download('figure19_code_challenge_20.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "b3yM18oxmxOt",
        "outputId": "51c87f76-19cd-4e79-d8f0-dc5c523d2977"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "train_conf = skm.confusion_matrix(train_loader.dataset.tensors[1],train_preds,normalize='true')\n",
        "test_conf  = skm.confusion_matrix(test_loader.dataset.tensors[1],test_preds,normalize='true')\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,ax = plt.subplots(1,2,figsize=(phi*6,6),constrained_layout=True)\n",
        "\n",
        "vmin = min(train_conf.min(),test_conf.min())\n",
        "vmax = max(train_conf.max(),test_conf.max())\n",
        "\n",
        "ax[0].imshow(train_conf,'Blues',vmin=vmin,vmax=vmax)\n",
        "ax[0].set_xticks(range(10))\n",
        "ax[0].set_yticks(range(10))\n",
        "ax[0].set_title('Train confusion matrix')\n",
        "ax[0].set_xlabel('True number')\n",
        "ax[0].set_xlabel('Predicted number')\n",
        "ax[0].set_ylabel('True number')\n",
        "\n",
        "img = ax[1].imshow(test_conf,cmap='Blues',vmin=vmin,vmax=vmax)\n",
        "ax[1].set_xticks(range(10))\n",
        "ax[1].set_yticks(range(10))\n",
        "ax[1].set_title('Train confusion matrix')\n",
        "ax[1].set_xlabel('Predicted number')\n",
        "ax[1].set_ylabel('True number')\n",
        "fig.colorbar(img,ax=ax[1],shrink=0.6)\n",
        "\n",
        "plt.savefig('figure20_code_challenge_20.png')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "files.download('figure20_code_challenge_20.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "-BKlrATWqhiu"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 1\n",
        "#    The overall and 7-specific performance of the model is still pretty good. What if you reduce the number of 7's to\n",
        "#    100 instead of 500? What does this tell you about the categorizability of the MNIST set?\n",
        "\n",
        "# The overall performance is still quite good, while the recall tends to drop\n",
        "# even more; overall, the categorisability of the MNIST data set seems to be\n",
        "# quite solid even for unbalanced designs (limited to one class), however, the\n",
        "# items from the small class tend to be misattributed to other classes, so that\n",
        "# the model performs good on the \"balanced classes\", but not on the \"unbalanced\"\n",
        "# ones\n",
        "\n",
        "# Re-run with fewer samples\n",
        "train_loader,test_loader = get_dataset(digits=[7],n_per_digits=100)\n",
        "train_acc,test_acc,losses,ANN = train_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "ZqsTkQuKqhgJ"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 2\n",
        "#    Perhaps it's still pretty easy to learn the digits when only one category has a small N. In a for-loop, reduce the\n",
        "#    number of the first 5 digits to N=100, while leaving the other 5 digits at their full sample size. Does that affect\n",
        "#    model performance?\n",
        "\n",
        "# Surprisingly or not, the overall performance is still quite good; as for the\n",
        "# reduced samples, the various metrics do decrease, but differently, some digits\n",
        "# are more robust while other are more susceptible; my guess (based also on the\n",
        "# additional exploration from the previous video), is that some digits are\n",
        "# simply easier to classify, probably because they are written down in a more\n",
        "# consistent way (e.g., 0, 1) as compared to others (e.g., 3, 4), and\n",
        "# similarities in digit shapes of course also contribute to wrong classification\n",
        "\n",
        "# Re-run with fewer samples for more digits (heavily unbalanced design)\n",
        "train_loader,test_loader = get_dataset(digits=[0,1,2,3,4],n_per_digits=100)\n",
        "train_acc,test_acc,losses,ANN = train_model()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
