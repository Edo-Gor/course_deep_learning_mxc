{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXpeehXZypvO"
      },
      "outputs": [],
      "source": [
        "# %% Deep learning - Section 16.153\n",
        "#    Denoising MNIST\n",
        "\n",
        "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
        "#   > https://www.udemy.com/course/deeplearning_x\n",
        "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
        "# from code developed by the course instructor (Mike X. Cohen), while the\n",
        "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
        "# creative input from my side. If you are interested in DL (and if you are\n",
        "# reading this statement, chances are that you are), go check out the course, it\n",
        "# is singularly good.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qOF67jQJzQGb"
      },
      "outputs": [],
      "source": [
        "# %% Libraries and modules\n",
        "import numpy               as np\n",
        "import matplotlib.pyplot   as plt\n",
        "import torch\n",
        "import torch.nn            as nn\n",
        "import seaborn             as sns\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "import pandas              as pd\n",
        "import scipy.stats         as stats\n",
        "import sklearn.metrics     as skm\n",
        "import time\n",
        "\n",
        "from torch.utils.data                 import DataLoader,TensorDataset\n",
        "from sklearn.model_selection          import train_test_split\n",
        "from google.colab                     import files\n",
        "from torchsummary                     import summary\n",
        "from scipy.stats                      import zscore\n",
        "from IPython                          import display\n",
        "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg')\n",
        "plt.style.use('default')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "xVOgLYVjRHVJ"
      },
      "outputs": [],
      "source": [
        "# %% Data\n",
        "\n",
        "# Load data\n",
        "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
        "\n",
        "# Split labels from data\n",
        "labels = data[:,0]\n",
        "data   = data[:,1:]\n",
        "\n",
        "# Normalise data (original range is (0,255))\n",
        "data_norm = data / np.max(data)\n",
        "\n",
        "# Convert to tensor\n",
        "data_tensor = torch.tensor(data_norm).float()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "IoGYwMrDRHSo"
      },
      "outputs": [],
      "source": [
        "# %% Model class\n",
        "\n",
        "# No need to create train and test datasets!\n",
        "\n",
        "def gen_model():\n",
        "\n",
        "    class mnist_AE(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            # Architecture\n",
        "            self.input  = nn.Linear(784,250)\n",
        "            self.encode = nn.Linear(250, 50)\n",
        "            self.mid    = nn.Linear( 50,250)\n",
        "            self.decode = nn.Linear(250,784)\n",
        "\n",
        "        # Forward propagation (sigmoid to scale between 0 and 1)\n",
        "        def forward(self,x):\n",
        "\n",
        "            x = F.relu(self.input(x))\n",
        "            x = F.relu(self.encode(x))\n",
        "            x = F.relu(self.mid(x))\n",
        "            x = torch.sigmoid(self.decode(x))\n",
        "\n",
        "            return x\n",
        "\n",
        "    # Generate model instance\n",
        "    ANN = mnist_AE()\n",
        "\n",
        "    # Loss function\n",
        "    loss_fun = nn.MSELoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(ANN.parameters(),lr=0.001)\n",
        "\n",
        "    return ANN,loss_fun,optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh9UBZAaRHLr",
        "outputId": "ee7a65ac-802e-4915-f591-0b2ffad1fe72"
      },
      "outputs": [],
      "source": [
        "# %% Test the model\n",
        "\n",
        "ANN,loss_fun,optimizer = gen_model()\n",
        "\n",
        "X    = data_tensor[:5,:]\n",
        "yHat = ANN(X)\n",
        "\n",
        "print(X.shape)\n",
        "print(yHat.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "PRm7rGrQRMx8",
        "outputId": "7c193545-5dde-4d0b-e85b-4d34706f2cc7"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,axs = plt.subplots(2,5,figsize=(1.5*phi*5,5))\n",
        "\n",
        "for i in range(5):\n",
        "    axs[0,i].imshow(X[i,:].view(28,28).detach() ,cmap='gray')\n",
        "    axs[1,i].imshow(yHat[i,:].view(28,28).detach() ,cmap='gray')\n",
        "    axs[0,i].set_xticks([]), axs[0,i].set_yticks([])\n",
        "    axs[1,i].set_xticks([]), axs[1,i].set_yticks([])\n",
        "\n",
        "plt.suptitle('Auch!\\n(bad pre-training performance)')\n",
        "\n",
        "plt.savefig('figure1_denoising_mnist.png')\n",
        "plt.show()\n",
        "files.download('figure1_denoising_mnist.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "NXVIdyYwRMsA"
      },
      "outputs": [],
      "source": [
        "# %% Function to train the model\n",
        "\n",
        "def train_model():\n",
        "\n",
        "    # Model instance\n",
        "    ANN,loss_fun,optimizer = gen_model()\n",
        "\n",
        "    # Parameters, inizialise vars\n",
        "    num_epochs = 10000\n",
        "    losses     = []\n",
        "\n",
        "    # Loop over epochs (no minibatch loop)\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Select a random subset of images\n",
        "        random_i = np.random.choice(data_tensor.shape[0],size=32)\n",
        "        X        = data_tensor[random_i,:]\n",
        "\n",
        "        # Forward propagation and loss (note how here the loss is not taking the\n",
        "        # correct labels, but rather the data themselves, the same data that\n",
        "        # have been input into the model)\n",
        "        yHat = ANN(X)\n",
        "        loss = loss_fun(yHat,X)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Loss in this epoch\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    return losses,ANN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "K9RWgA71UbwH",
        "outputId": "69580a95-2dd8-4d09-b5a4-2ca295729035"
      },
      "outputs": [],
      "source": [
        "# %% Train and plot losses\n",
        "\n",
        "# Train (takes ~2 mins)\n",
        "losses,ANN = train_model()\n",
        "print(f'Final loss: {losses[-1]:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "DfHQosqeUbsz",
        "outputId": "8ca14c71-6d2e-4017-8f21-f2d9c77fe060"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "plt.figure(figsize=(phi*5,5))\n",
        "\n",
        "plt.plot(losses,'-')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Model loss')\n",
        "plt.title('Model loss over epochs')\n",
        "\n",
        "plt.savefig('figure2_denoising_mnist.png')\n",
        "plt.show()\n",
        "files.download('figure2_denoising_mnist.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "uDwTSNt_UbqO",
        "outputId": "705c8f61-77e9-4780-955b-f3dffdf48c9b"
      },
      "outputs": [],
      "source": [
        "# %% Plotting\n",
        "\n",
        "X = data_tensor[:5,:]\n",
        "yHat = ANN(X)\n",
        "\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,axs = plt.subplots(2,5,figsize=(1.5*phi*5,5))\n",
        "\n",
        "for i in range(5):\n",
        "    axs[0,i].imshow(X[i,:].view(28,28).detach() ,cmap='gray')\n",
        "    axs[1,i].imshow(yHat[i,:].view(28,28).detach() ,cmap='gray')\n",
        "    axs[0,i].set_xticks([]), axs[0,i].set_yticks([])\n",
        "    axs[1,i].set_xticks([]), axs[1,i].set_yticks([])\n",
        "\n",
        "plt.suptitle('Post-training performance')\n",
        "\n",
        "plt.savefig('figure3_denoising_mnist.png')\n",
        "plt.show()\n",
        "files.download('figure3_denoising_mnist.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "NYEzfPNvXutH",
        "outputId": "2012d0fe-e68d-4eb7-d8e5-4306ba16e3e9"
      },
      "outputs": [],
      "source": [
        "# %% A common use of autoencoders (denoising)\n",
        "\n",
        "# Get a small set of images and add uniform noise to simulate a noisy input\n",
        "X       = data_tensor[:10,:]\n",
        "X_noise = X + torch.rand_like(X)/4\n",
        "\n",
        "# clip at 1 to maintain normalisation\n",
        "X_noise[X_noise>1] = 1\n",
        "\n",
        "# Plotting\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,axs = plt.subplots(2,5,figsize=(1.5*phi*5,5))\n",
        "\n",
        "for i in range(5):\n",
        "    axs[0,i].imshow(X[i,:].view(28,28).detach() ,cmap='gray')\n",
        "    axs[1,i].imshow(X_noise[i,:].view(28,28).detach() ,cmap='gray')\n",
        "    axs[0,i].set_xticks([]), axs[0,i].set_yticks([])\n",
        "    axs[1,i].set_xticks([]), axs[1,i].set_yticks([])\n",
        "\n",
        "plt.suptitle('Noisy data')\n",
        "\n",
        "plt.savefig('figure4_denoising_mnist.png')\n",
        "plt.show()\n",
        "files.download('figure4_denoising_mnist.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "DrfzUK9jXuho",
        "outputId": "2110f0af-d728-49a0-9166-a59e80022f23"
      },
      "outputs": [],
      "source": [
        "# %% Run the model on simulated noisy data\n",
        "\n",
        "# Model pass\n",
        "Y = ANN(X_noise)\n",
        "\n",
        "# Plotting\n",
        "phi = (1 + np.sqrt(5)) / 2\n",
        "fig,axs = plt.subplots(3,10,figsize=(1.5*phi*5,5))\n",
        "\n",
        "for i in range(10):\n",
        "    axs[0,i].imshow(X[i,:].view(28,28).detach(),cmap='gray')\n",
        "    axs[1,i].imshow(X_noise[i,:].view(28,28).detach(),cmap='gray')\n",
        "    axs[2,i].imshow(Y[i,:].view(28,28).detach(),cmap='gray')\n",
        "    axs[0,i].set_xticks([]), axs[0,i].set_yticks([])\n",
        "    axs[1,i].set_xticks([]), axs[1,i].set_yticks([])\n",
        "    axs[2,i].set_xticks([]), axs[2,i].set_yticks([])\n",
        "\n",
        "plt.suptitle('Look at that, a bit distorted but nice for such a small model.')\n",
        "\n",
        "plt.savefig('figure5_denoising_mnist.png')\n",
        "plt.show()\n",
        "files.download('figure5_denoising_mnist.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "fUz5uMNLaOZd"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 1\n",
        "#    Because these are continuous data, mean-squared-error is the correct loss function. But I mentioned in the previous\n",
        "#    video that binary cross-entropy loss is *sometimes* used in autoencoders. Does the loss function make a difference\n",
        "#    for this problem? Why?\n",
        "\n",
        "# Surprisingly, and somewhat puzzling, the reconstructed data are not great even\n",
        "# though the loss of the model decreases over epochs;Â that said, you can kind of\n",
        "# see the digits floating around in a cloud\n",
        "\n",
        "# Binarise data (optional)\n",
        "data   = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
        "labels = data[:,0]\n",
        "data   = data[:,1:]\n",
        "\n",
        "data_norm   = data / np.max(data)\n",
        "data_norm   = np.where (data_norm>.5,1,0)\n",
        "data_tensor = torch.tensor(data_norm).float()\n",
        "\n",
        "# Model class\n",
        "def gen_model():\n",
        "\n",
        "    class mnist_AE(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            self.input  = nn.Linear(784,250)\n",
        "            self.encode = nn.Linear(250, 50)\n",
        "            self.mid    = nn.Linear( 50,250)\n",
        "            self.decode = nn.Linear(250,784)\n",
        "\n",
        "        # Forward propagation (remove sigmoid, implemented in BCEWithLogitsLoss)\n",
        "        def forward(self,x):\n",
        "\n",
        "            x = F.relu(self.input(x))\n",
        "            x = F.relu(self.encode(x))\n",
        "            x = F.relu(self.mid(x))\n",
        "            x = self.decode(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "    # Generate model instance\n",
        "    ANN       = mnist_AE()\n",
        "    loss_fun  = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(ANN.parameters(),lr=0.001)\n",
        "\n",
        "    return ANN,loss_fun,optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5On7c2xZaOXN"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 2\n",
        "#    Change the number of units in the latent layer to 10. How does that affect the loss and denoising? Don't turn this\n",
        "#    into a full parametric experiment -- that's for the next video!\n",
        "\n",
        "# The loss is a bit higer that the basic model, and indeed the reconstructed\n",
        "# data are not as good; the denoising also breaks down even though some shapes\n",
        "# are still recognisable, while some shapes are clearly the results of a feature\n",
        "# mixing. Quite interesting because we know that the MNIST dataset contains 10\n",
        "# categories, the 10 digits, so one could hypothesise that 10 dimentions are\n",
        "# enough to make the model differentiate between them; and yet it seems that\n",
        "# more than 10 dimentions are necessary to get a decent performance (\"decent\" =\n",
        "# eyeballing some data), suggesting that for each digit there is more than one\n",
        "# relevant dimention (and maybe some of them are even non-orthogonal, e.g., see\n",
        "# how the 5s look like 8s)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "o33nfT2TaOVB"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 3\n",
        "#    The code here picks samples randomly, which means many samples are skipped, and some could be repeated. Change the\n",
        "#    code so that the model goes through every item exactly once per epoch. The order should be randomized to avoid\n",
        "#    possible order effects. You'll probably want to reduce the number of epochs!\n",
        "\n",
        "# The basic peformance increases quite neatly! And even the denoising part does,\n",
        "# despite some \"inaccuracies\"\n",
        "\n",
        "# Create a DataLoader\n",
        "dataset     = TensorDataset(data_tensor)\n",
        "data_loader = DataLoader(dataset,batch_size=32,shuffle=True,drop_last=False)\n",
        "\n",
        "# Function to train the model\n",
        "def train_model():\n",
        "\n",
        "    ANN,loss_fun,optimizer = gen_model()\n",
        "\n",
        "    num_epochs = 50\n",
        "    losses     = []\n",
        "\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        batch_loss = []\n",
        "\n",
        "        for (X,) in data_loader:\n",
        "\n",
        "            yHat = ANN(X)\n",
        "            loss = loss_fun(yHat,X)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "        losses.append( np.mean(batch_loss) )\n",
        "\n",
        "    return losses,ANN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "D_CSaRj5aOSZ"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 4\n",
        "#    Is it necessary to normalize the data to a range of [0 1]? What are arguments for normalization, and arguments why\n",
        "#    it may not be critical (in this problem)?\n",
        "\n",
        "# I'd say data normalisation is quite necessary, otherwise model fitting blows\n",
        "# up completely, probably due to the numerical instability in weight adjustment\n",
        "# (e.g., large values in the data might just produce large losses that are not\n",
        "# actually related to the importance of a feature)\n",
        "\n",
        "# Non-normalised data\n",
        "data   = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
        "labels = data[:,0]\n",
        "data   = data[:,1:]\n",
        "\n",
        "data_tensor = torch.tensor(data).float()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zp_kNOjmaOL9"
      },
      "outputs": [],
      "source": [
        "# %% Exercise 5\n",
        "#    The autoencoder did a pretty decent job at denoising the images. How far can you push this? Try adding more noise\n",
        "#    to the images and re-running the test code (you don't need to retrain the model). Is the autoencoder robust to a\n",
        "#    a larger amount of noise?\n",
        "\n",
        "# What is large is relative, but the performace can deteriorate quite quickly and\n",
        "# drops completely when the noise has the same magnitude as the data\n",
        "\n",
        "# Trying various noise levels (double, half, etc.)\n",
        "X_noise = X + torch.rand_like(X)/2\n",
        "X_noise = X + torch.rand_like(X)/8\n",
        "X_noise = X + torch.rand_like(X)/1\n",
        "X_noise = X + torch.rand_like(X)/12\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
